 %!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\def \papersize {a4paper}

\documentclass[12pt,\papersize]{extarticle}
% extarticle is like article but can handle 8pt, 9pt, 10pt, 11pt, 12pt, 14pt, 17pt, and 20pt text

\def \ititle {Shared Agency Involves Changing Perspective: A Counterexample to Bratman}
\def \isubtitle {}
\def \iauthor {Stephen A.\ Butterfill}
\def \iemail{s.butterfill@warwick.ac.uk}
%\date{}

\input{$HOME/Documents/submissions/preamble_steve_paper3}
%\author{}
%\date{}



%\setromanfont[Mapping=tex-text]{Sabon LT Std} 

\begin{document}

\setlength\footnotesep{1em}

\bibliographystyle{$HOME/Documents/submissions/mynewapa} %apalike

\maketitle
%\tableofcontents
\title{}

\begin{abstract}
\noindent
The leading, best developed account of shared agency, Michael Bratman's, hinges on the claim that, roughly,
we have a shared intention if we each intend that we J and also that we J by way of these intentions and meshing subplans of them where this is all common knowledge.
This paper provides a counterexample to the sufficiency of this condition for shared intention.
The counterexample arises because it is possible for agents to meet the condition, and to act rationally on the specified knowledge and intentions,
 while conceiving of each other's actions only as constraints to work around and opportunities to exploit.
Shared agency requires more than this, or so we argue.
We also suggest a way of revising Bratman's account  that is consistent with his general approach.
In some or all cases, shared agency differs from individual agency in part because shared agency involves changing perspective so that one’s own and other’s actions are conceived of as parts of a single plan.

\end{abstract}

\section{Shared Intention}
Why, if at all, is a notion of shared intention needed? 
This question is standardly answered by appeal to contrast cases \citep[compare][p.\ 150]{Bratman:2009lv}.
Thus \citet{gilbert_walking_1990} contrasts friends intentionally walking together with two people who happen to be walking side by side. 
And \citet{Searle:1990em} contrasts park visitors who  simultaneously run to a central shelter in performing a dance with park visitors who likewise run to the central shelter but only because of an impending storm.
%And Bratman (\citeyear{Bratman:1992mi}, p.\ 333; \citeyear{Bratman:1993je}, p.\ 103--4) contrasts two people who cooperatively plan to go to New York  together with two people who each have an intention that they go to New York together by way of coercing the other into travelling with her.
These and other contrast cases invite the question, 
How do cases involving shared agency differ from cases involving only  parallel individual agency? 

The first contrast case, Gilbert's, shows that the difference can’t be just  a matter of coordination because people who are merely happen to be walking side by side each other also need to coordinate their actions in order to avoid colliding.  
Note also that in both cases each individual's walking is intentional, so our intentionally walking together cannot be  only a matter of our each intentionally walking.
The second contrast case, Searle's, shows that the difference can’t just be that the resulting actions have a common effect because merely parallel actions can have common effects too.%
\footnote{
This use of contrast cases resembles \citet{Pears:1971fk}: he uses contrast cases to argue that whether something is an ordinary, individual action depends on its antecedents. 
} 
Perhaps, then, a notion of shared intention is needed to distinguish the two cases.  
Perhaps it is our acting on a shared intention that we walk together which distinguishes us from two strangers who happen to be walking side by side.%
\footnote{
Many philosophers agree that a notion of shared intention is useful for understanding acting together. 
Compare \citet[p.\ 5]{Gilbert:2006wr}: `I take a collective action to involve a collective intention.'  See also  
	\citet[p.\ 381]{Carpenter:2009wq}, 
	\citet[p.\ 369]{Call:2009fk}, 
	\citet{Kutz:2000si}, 
	\citet[p.\ 117]{rakoczy_pretend_2006} and 
	\citet{Tollefsen:2005vh}.
	}
	

But what could shared intention be?
In an influential series of papers,\footnote{ 
See \citet{Bratman:1992mi,Bratman:1993je,Bratman:1999fr,Bratman:2009lv}.
For influences beyond philosophy, see e.g.\ \citet{Tomasello:2005wx} and \citet{Knoblich:2008hy}. 
}
Bratman claims that the following are collectively sufficient\footnotemark \ conditions for you and I to have a shared intention that we J:
%
\footnotetext{
In \citet{Bratman:1992mi}, the following were offered as jointly sufficient \textit{and individually necessary} conditions; the retreat to sufficient conditions occurs in \citet[][pp.\ 143-4]{Bratman:1999fr} where he notes that `for all that I have said, shared intention might be multiply realizable.'
} 
%
\begin{quote}
\label{quote:bratman_account}
`1. (a) I intend that we J and (b) you intend that we J
 
`2. I intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb
 
`3. 1 and 2 are common knowledge between us' \citep[][p.\ View 4]{Bratman:1993je}.
\end{quote}
%
In this paper we give a counterexample to Bratman's  claim that the above conditions, (1)--(3), are collectively sufficient conditions for shared intention. 
We shall also suggest a revision to avoid the counterexample.
%Apart from improving our understanding of what shared intention is, this will also bear on the kinds of planning that are involved in acting together.


Before going further we must distinguish two versions of the claim that (1)--(3) are collectively sufficient for shared intention.
The \emph{weak claim} is that there is some J such that these conditions are sufficient for you and I to intend that we J.
The \emph{strong claim} is that for any J, these conditions are sufficient for you and I to intend that we J. 
Our initial counterexample will be directed to the strong claim.
However, after we have constructed this counterexample it will become clear that there are also counterexamples involving a wide range mundane activities to which Bratman explicitly takes his account to apply.
Even walking together%
	\footnote{
	See \citet[p.\ 150]{Bratman:2009lv}.
	}
	%
and painting a house together%
	\footnote{
	See \citet[p.\ 331]{Bratman:1992mi}.
	}
	%
can serve as counterexamples.
%One of Bratman's aims is to show that an account of shared intention should be \emph{conceptually conservative}.
%He aims, that is, to show that  it is possible to give an account of shared intention using only concepts that `are available within the theory of individual planning agency' \citep[p.\ 163]{Bratman:2009lv}.  
%Achieving this aim would require the strong claim, and it is to the strong claim that our counterexample is directed.%
%\footnote{
%We do not aim to attack the claim that it is possible to give a conceptually conservative account of shared intention. 
%The point we are making here is just that if an account
%}
 



How could we show that our meeting Bratman's conditions, (1)--(3), is not in fact sufficient for us to have a shared intention? 
We seek a case where the conditions are met although we lack a shared intention.
But how could we determine that we lack a shared intention?
As already mentioned, the notion of shared intention is supposed to 
	make it possible to 
	characterise systematically a difference between 
		cases involving shared agency (such as our walking together)
		and
		cases involving only parallel  individual agency (such as two strangers who happen to be walking the same route side-by-side). 
Suppose, then, that we had a trio of cases, A, B and C, each involving two agents.
Suppose, further, that A and B involved parallel agency only, whereas C involved shared agency.
Then we could be sure that A and B do not involve shared intention.
Now suppose 
	that, for some relevant J, Bratman's conditions, (1)--(3) above, were met in cases B and C alike, 
	that in each case the structure of intention and knowledge played an appropriate role in guiding the agents' actions,
	and that in each case the agents thereby successfully J.
Then case B would be our counterexample:
Bratman's conditions are met but there is no shared intention.
This is how our counterexample will work.
(Strictly speaking, case A is not necessary; but including it simplifies exposition.)

Several preliminaries are necessary for the construction of our counterexample. 
These preliminaries might easily give the impression that our counterexample depends on an artificial settings. 
However, having introduced the primary counterexample in an artificial setting, we will  go on to show that counterexamples can also be constructed for mundane activities including walking together.


%%
%%two agents who intentionally J together (e.g.\ walk  together) and two agents who also  J together but do not do so intentionally (e.g.\ they happen to be walking side by side). 
%%
%This has two consequences.
%First, it should be impossible to construct a pair cases, A and B,
%meeting these criteria:
%	(i) in each case, two agents act intentionally;
%	(ii) in the first case, the two agents do not meet conditions (1)--(3) above;
%	(iii) in the second case, the agents do meet conditions (1)--(3) above;
%	(iv) the cases, A and B, are otherwise as similar as possible;
%	and 
%	(v) neither A nor B exemplifies the sort of shared agency gestured at by appeal to the contrast cases we started with.
%Second, it should also be impossible to construct a pair cases, B and C,
%meeting these criteria:
%	(i$^\prime$) in each case, two agents act intentionally;
%	(ii$^\prime$) in each case, the agents do meet conditions (1)--(3) above;
%	and 
%	(iii$^\prime$) C but not B exemplifies the sort of shared agency gestured at by appeal to the contrast cases we started with.
%
%
%
%
%Let's stipulate that two cases \emph{differ with respect to shared agency} when they differ in the way exemplified by the contrast between our intentionally walking together and our merely walking in parallel.
%Suppose, then, that we construct three cases, A, B and C.
%The three cases are as similar as possible, except that the above conditions are met in cases B and C but not in case A.
%Now if Bratman's conditions are sufficient for shared intention, A should differ from B with respect to shared agency, and B should not differ from C with respect to shared agency.
%But suppose that in fact A does not differ from B with respect to shared agency, whereas B does so differ from C.%
%\footnote{
%Strictly speaking only A and B are needed for the counterexample, of course.
%But including C makes it clearer that A and B do not differ with respect to shared agency.
%}
%Then we can conclude that Bratman's conditions are not sufficient for shared intention.
%This is how our counterexample will work.


\section{Neutral with Respect to Shared Intentionality}
\label{sec:netural_wrt_shared_intentionality}
Before we can introduce the counterexample 
we need to highlight a feature of Bratman's account 
	which we shall be exploiting.

Consider the contents of the intentions concerning our J-ing in the above clauses, (1)--(3). 
What sort of activity can you intend when you intend that we J?
We cannot restrict possible values of J to activities which involve shared agency.
In imposing any such restriction we would be assuming the very notion that an account of shared intention is supposed to illuminate. 
%
%As we have stressed, 
%	make it possible to 
%	characterise systematically a difference between 
%		cases involving shared agency (such as our walking together)
%		and
%		cases involving parallel agency only (such as two strangers who happen to be walking the same route side-by-side). 
%We must therefore avoid tacitly appealing to this distinction by  restricting possible values of J to those which involve shared agency. 
Rather the above  conditions, (1)--(3), must be sufficient for shared intention even for some values of J which are `neutral with respect to shared intentionality'.%
\footnote{
 \citet[p.\ 147]{Bratman:1999fr}.
 This refines Bratman's earlier view that some admissable values of J are cooperatively neutral 
 	where an  act-type is \emph{cooperatively neutral} just if `joint performance of an act of that type may be cooperative, but it need not be' \citep[p.\ 330]{Bratman:1992mi}. 
}

To illustrate, first consider Bratman's `mafia case' where two people go to New York together by virtue of one of them forcing the other into a car and driving off \citep[p.\ 333]{Bratman:1992mi}. 
Next consider a paradigm case of where two friends cooperatively go to New York together.
It's possible to conceive of an act type of going to New York together which is broad enough that both cases, the coercive and the cooperative, fall under it.
And it's even possible to desire or intend to be involved in an act of this type.
A gangster might intend to go to New York with a future victim without having yet decided whether they will do this cooperatively or coercively.

A consequence is that, in the right situations, one of us can rationally intend that we J, and can intend this unilaterally, that is without depending on anyone else intending that we J. 
Suppose you know that I am going to New York via a certain route at a particular time, 
and that I will do this regardless of what you do.
Suppose also that you can rationally intend that you go to New York in the same manner,
and that you know that if you act on this intention the upshot will be that we will go to New York together (although I may not know that we are going together---perhaps you will conceal your presence from me).
Then you can rationally intend that we go to New York together, 
	and you can intend this irrespective of whether I have any corresponding intention---providing, of course, that in so intending you are conceiving of our going to New York together in a way that is neutral with respect to shared intentionality.
What follows depends on the premise that this is indeed possible.%
\footnote{ 
\citet{Bratman:1999fr}  defends this claim at length. 
Note also that this claim must be true if Bratman's account of shared intention is to provide an informative and systematic distinction between the contrast cases mentioned at the start.
}


\section{Unshared Intentions}
As a further preliminary we need to introduce a definition.
Let us stipulate that we have an \emph{unshared intention} that we <J$_1$, J$_2$> where J$_1$$\neq$J$_2$ just if:
%
\begin{quote}
\label{df:unshared_intention}
1$^\prime$. (a) I intend that we J$_1$ and (b) you intend that we J$_2$
 
2$^\prime$. I intend that we J$_1$ in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J$_2$ in accordance with and because of la, lb, and meshing subplans of la and lb
 
3$^\prime$. 1 and 2 are common knowledge between us.
\end{quote}
In defining unshared intention we have used conditions exactly like Bratman's sufficient conditions for shared intention except that Bratman's conditions have J$_1$$=$J$_2$.
At this point it might be natural for readers to suppose that agents could not have unshared intentions,
or at least that they could not do so without irrationality.
In this section we describe a possible situation in which two agents  have an unshared intention without irrationality, deception or even ignorance.
This possible situation is not the promised counterexample, but it does form the basis for it.

Let us first introduce the activity we shall focus on.
Ayesha and Ahmed  are playing a simple video game which involves moving a cross around a two-dimensional space littered with barriers.
Ayesha can only accelerate the cross backwards or forwards,
while Ahmed can only accelerate it left or right. 
The cross moves around and interacts with the barriers in ways both players can predict.
The players are given tasks independently. 
These tasks always involve making the cross hit a target within two minutes of starting. 
A player succeeds when the cross hits her target, regardless of what happens to the cross afterwards.  
(It may go on to hit another target.)
In this case, 
	Ayesha's task is to make the cross hit the red square
	while
	Ahmed's task is make the cross hit the blue circle. 
In general it is possible that either or both will succeed, or that they will both fail.
Each movement carries a small cost to the player who moves, so that Ayesha and Ahmed each attempt to minimize how much he or she moves the cross consistently with completing his or her task.
At the outset, 
Ayesha and Ahmed are each neutral on whether the other succeeds or fails.
They are not opponents and do not seek to undermine each other's efforts, but each is entirely concerned  with his or her own task.
All of this is common knowledge for Ayesha and Ahmed.
They both know who has which task, what constraints they face and what their motives are.

Consider the possibility of one player intending, unilaterally, that the two players do something.
Suppose that one of the players---Ayesha, say---can knowledgeably predict that if she performs a certain sequence actions, <a$_1$, a$_2$, ...\ a$_n$>, then Ahmed will simultaneously perform certain other actions, <b$_1$, b$_2$, ...\ b$_n$>,
 and the upshot will be that the cross hits the red square.
Were this to happen, it would be true that Ayesha and Ahmed made the cross hit the red square.
Suppose, further, that Ayesha can intend to perform those actions <a$_1$, a$_2$, ...\ a$_n$>.
Then Ayesha can intend, unilaterally, that they, Ayesha and Ahmed, make the cross hit the red square (see Section \vref{sec:netural_wrt_shared_intentionality}).

Unshared intentions require a kind of symmetry.
Let us suppose that the above sequences of actions,
	Ayesha's <a$_1$, a$_2$, ...\ a$_n$>  and 
	Ahmed's <b$_1$, b$_2$, ...\ b$_n$>,
will also result in the cross hitting the blue circle. 
(Since the cross has momentum, we can suppose that it will hit both the red square and the blue circle at some time after these action sequences have been performed.)
Then by the reasoning just offered, Ahmed could intend that they, Ayesha and Ahmed make the cross hit the blue circle.
So Ayesha and Ahmed could meet the first condition, (1), for having an unshared intention.

What about the second condition, (2)?
Suppose that Ayesha knows two further things.
First, that Ahmed intends that they, Ayesha and Ahmed, make the cross hit the blue circle.
Second, that in acting on his intention Ahmed will perform actions <b$_1$, b$_2$, ...\ b$_n$>.
Then Ayesha can intend that they, Ayesha and Ahmed, make the cross hit the red square in accordance with and because of her intention that they make the cross hit the red square and in accordance with and because of Ahmed's intention that they make the cross hit the blue circle.

This is not quite enough to meet the second condition, (2), because there is also a requirement about meshing subplans. 
To apply this requirement we need to generalise Bratman's definition of meshing:
\begin{quote}
`our individual subplans concerning our J-ing \emph{mesh} just in case there is some way we could J that would not violate either of our subplans but would, rather, involve the successful execution of those subplans' \citep[p.\ 106]{Bratman:1993je}.
\end{quote}
A natural generalisation is this:
\begin{quote}
our individual subplans concerning our <J$_1$, J$_2$>-ing \emph{mesh} just in case there is some way I could J$_1$ and you could J$_2$ that would not violate either of our subplans but would, rather, involve the successful execution of those subplans. 
\end{quote}
%
To illustrate, 
their subplans would fail to mesh if, in intending that they make the cross hit the red square,  Ayesha's plans had included pushing Ahmed out of the way and seizing his controls. 
They would also fail to mesh if Ayesha were planning to trick Ahmed into a situation where we would be unable to perform the actions he had been planning.
But in the case we have been describing, the agents subplans mesh perfectly.
Each agent's subplans involve manipulating his or her own controls,
and the successes each seeks in doing this depends on the other successfully carrying out their subplans.
So Ayesha can rationally intend that they, Ayesha and Ahmed, make the cross hit the red square in accordance with and because of their intentions and meshing subplans of them.
And Ahmed likewise for making the cross hit the blue circle.

The only outstanding requirement for Ayesha and Ahmed to have an unshared intention is that their various intentions are common knowledge. 
Assuming common knowledge is possible concerning Bratman's conditions (1)--(2) on page \pageref{quote:bratman_account}, 
it is likewise possible for the corresponding conditions on unshared intention, (1$^\prime$)--(2$^\prime$) on page  \pageref{df:unshared_intention}.
So Ayesha and Ahmed can have an unshared intention that they <J$_1$, J$_2$> where J$_1$ is Ayesha and Ahmed's making the cross hit the red square and J$_2$ is their making the cross hit the blue circle.

So far we have shown that it is possible for two agents to have an unshared intention without irrationality, deception or ignorance.
Of course unshared intentions may be rare. 
But what matters for our counterexample is just that they are possible. 



%\section{Pure Unshared Intentions}
%We stipulate that an unshared intention is \emph{pure} if having that unshared intention does not involve having any shared intention. 
%It may be tempting to assume that all unshared intentions are pure. 
%This is not obvious, however, and may not even be true.
%Since Bratman provides only sufficient conditions for shared intention, his account doesn't tell us that an unshared intention is not a shared intention.
%For all Bratman says, the conditions defining unshared intention might also be collectively sufficient conditions for shared intention. 
%
%But doesn't shared agency require that there be a single activity, J-ing, about which each the agent involved has an intention? 
%This, too, is not obvious.  It might reasonably be denied by those who, like Bratman, reject the Simple View according to which when an individual intentionally J-s she has an intention concerning her J-ing \citep{Bratman:1984jr}.
%So the purity of an unshared intention cannot be assumed without argument. 
%This is why we do not rely on this assumption in what follows.
%
%%
%How can we show that Ayesha and Ahmed's unshared intention that they <J$_1$, J$_2$>  is pure?
%Perhaps we can appeal to intuition.
%Ayesha sees Ahmed's actions as constraints on her own, or else as opportunities.
%She exploits Ahmed's intentions for her own ends.
%Of course the situation is reciprocal: Ahmed exploits Ayesha in equal measure.
%Each allows himself or herself to be exploited by the other because being exploited enables exploiting,
%and this is the full extent of their cooperation. 
%Perhaps it is clear enough that 
%the sort of shared agency that an account of shared intention is supposed to capture must involve more than this sort of reciprocal exploitation, where each agent sees the other's actions only as constraints or opportunities.%
%\footnote{
%Note that we are not suggesting that reciprocal exploitation is incompatible with shared agency.  
%The claim under consideration here is rather that shared agency requires more than reciprocal exploitation.
%} 
%If so, we can already claim that Ayesha and Ahmed have a pure unshared intention.
%
%
%
%
%
%We started this paper by explaining the need for a notion of shared intention by appeal to contrast cases (following Bratman). 
%It is contrast cases that are supposed give us a pre-theoretical handle on shared intention.
%So to show that Ayesha and Ahmed's unshared intention is pure, we need to construct a contrasting case.
%
%***Caitlin and Ciaran, 
%	like many people, 
% 	sometimes attribute intentions and other states to imaginary agents, and perform actions on their behalf. 
%In some cases this enables them to further their own real-world objectives (as in `Teddy wants to go to the park now').	
%Some of these imaginary agents are toys. 
%But some are imaginary aggregate agents. 
%Sometimes, that is, Caitlin and Ciaran imagine that there is an agent distinct from either of them and such that all its parts are parts of them.
%They attribute intentions and other states to this agent, and act on its behalf.
%
%
%


%
%An account of shared intention should enable us to  distinguish systematically between two types of case in which we J together, one where we intentionally J together and the other where  our J-ing together involves  merely parallel actions.
%As noted, Gilbert's example of walking together shows that merely parallel actions can be mutually responsive, as when two strangers walking side by side avoid collision thanks to meshing subplans.



\section{The Counterexample}
%*Need a beefier activity, something more than cooperatively neutral.

In the situation just described,
Ayesha and Ahmed are playing a game and have different tasks.
Thanks to special features of the game environment, they both succeed by acting on an unshared intention.
Now compare two further players, Beatrice and Baldric, who are playing the same game. 
Their situations, knowledge states, intentions and actions are as similar as possible to Ayesha's and Ahmed's except for one detail.
Just by chance they have been assigned identical tasks: Beatrice's task is to make the cross hit the red square and Baldric's task is the same.
So where Ayesha and Ahmed have an unshared intention that they <J$_1$, J$_2$>,
Beatrice and Baldric meet Bratman's conditions (1)--(3) for having a shared intention that they J$_1$.  
But Beatrice, in planning and acting, does not rely on the coincidence of their intentions; and nor does Baldric.
(Beatrice relies on the fact Baldric intends that they J$_1$, of course; but she does not rely on the fact that Baldric intends what she intends.)
Furthermore, due to an artefact of the way the game is structured,
the unshared intention and the 
	Bratman intention 
	\label{df:bratman_intention}
	(as we might label the structure of intention and knowledge while leaving open whether it constitutes a shared intention), result in the two pairs performing same actions in the same way.
That is, Beatrice reasons about Baldric much as Ayesha reasons about Ahmed and Beatrice and does what Ayesha does, and likewise for Baldric and Ahmed. 


We claim that Beatrice and Baldric have a shared intention that they J$_1$
only if 
Ayesha and Ahmed have a shared intention.%
\footnote{
Strictly speaking, 
	what matters for our argument is whether Ayesha and Ahmed have a shared intention \emph{in virtue of having the unshared intention that they <J$_1$, J$_2$>}.
	This is because, strictly speaking, we need to show, not that Beatrice and Baldric lack any shared intention whatsoever, but only that they lack a shared intention that they J$_1$ just in virtue of meeting Bratman's conditions (see (1)--(3) on page  \pageref{quote:bratman_account}). 
	For ease of exposition this is not made explicit in the main text.
} 
This claim follows from the similarities of the two cases.
The only difference is that Beatrice and Baldric happen to be assigned the same task, whereas Ayesha and Ahmed are not.
And neither Beatrice nor Baldric makes use of the fact that they have the same task. 
(This is not due to ignorance: it's just how they choose to approach their tasks.)
So if we consider on how  
		the case of Beatrice and Baldric
	differs from
		 that of Ayesha and Ahmed,
we can see that these differences do not plausibly amount to a difference with respect to shared agency.
Shared intention cannot feature in one case but not the other.

To show that Beatrice and Baldric do not have a shared intention it remains only for us to show that Ayesha and Ahmed do not have one. 
Here we must be careful.
First note that,
	 since Bratman provides only sufficient conditions for shared intention, 
	 his account doesn't tell us that an unshared intention is not a shared intention.
For all Bratman says, the conditions defining unshared intention might  be sufficient for shared intention. 

But doesn't shared intention require at least this much,
that there be a single activity  about which each the agent involved has an intention? 
This might reasonably be doubted by those who, like Bratman, reject the Simple View according to which when an individual intentionally F-s she has an intention concerning her F-ing \citep{Bratman:1984jr}.
So 
	we shall not infer  that Ayesha and Ahmed lack a shared intention
	just because (by construction) there is no F such that Ayesha and Ahmed each intend that they, Ayesha and Ahmed, F.

Can we then appeal directly to intuition to show that Ayesha and Ahmed lack a shared intention?
Ayesha sees Ahmed's actions as constraints on her own, or else as opportunities.
She exploits Ahmed's intentions for her own ends.
Of course the situation is reciprocal: Ahmed exploits Ayesha in equal measure.
Each allows himself or herself to be exploited by the other because being exploited enables exploiting,
and this is the full extent of their cooperation. 
We don't suppose that reciprocal exploitation is incompatible with shared intention. 
But Ayesha and Ahmed's interaction consists entirely in this sort reciprocal exploitation, where each agent sees the other's actions only as constraints or opportunities.
Perhaps it is clear enough that 
the sort of shared agency that an account of shared intention is supposed to capture must involve more than this. 
If so, we can already claim that Ayesha and Ahmed are not acting on a shared intention.
But philosophers' intuitions about shared agency may not be entirely, so it would be better if we could avoid such a blunt appeal to intuition.

How else could we support the claim that Beatrice and Baldric lack a shared intention? 
As mentioned at the start, 
the contrast cases are often used to anchor intuitions in theorising about shared agency.
In the next section we shall further support our claim by contrasting Beatrice and Baldric's case with a further case, one which is as similar as possible and which does seem to involve shared agency.
The fact that Beatrice and Baldric's case contrasts with this new case will support the claim that Beatrice and Baldric lack shared intention.
This is the aim of the following section.
A first motive for introducing this new contrast case is that it provides an indication concerning what is missing from Bratman's account, and so may help us to understand why his conditions are not sufficient for shared intention.



\section{Agent-neutral Plans}
\label{sec:distributed_plan}

Above we introduced what will turn out to be a counterexample to Bratman's account, the case of Beatrice and Baldric. 
But we have yet to show that this case really is a counterexample. 
For all we have said so far, a proponent of Bratman's view might insist that Beatrice and Baldric do have a shared intention that they J$_1$.
To show that they do not,
we shall contrast Beatrice and Baldric's case with a third case that is as similar as possible but does involve shared intention. 

%---the following is a mistake (because Beatrice and Baldric do take each other's task into account.
%It is unlikely that humans would spontaneously behave as Beatrice and Baldric do.%
%\footnote{
%This is nicely illustrated in a series of experiments by Sebanz and colleagues in which subject spontaneously take another's plans into account in planning their own actions, even where doing so is manifestly not required and can impair their performance  \citep{Sebanz:2003kf,Sebanz:2005fk,tsai:2011_groop_effect}.
%}

First we need to introduce the notion of an agent-neutral plan.
We stipulate that a planning process, or a plan, is \emph{agent-neutral} just if it does not involve identifying any particular agents.  
This sort of planning is quite common.
For example, some housemates who have decided to take on an allotment to grow vegetables might sit down together to plan what needs doing without yet assigning roles to particular individuals. 
In so planning, each housemate is thinking about what is to be done and not what she herself will do.  
%(This is an idealisation, of course.)
At some point the housemates stop planning.
(This does not necessarily mean that they have a fully worked out plan; like any other plans, agent-neutral plans can have gaps that may need filling in later.)
They now divide up the roles.
Of course they may not find a way of dividing up roles that everyone is prepared to go along with---individuals' preferences, abilities and intentions may block the plan's adoption.
But suppose the housemates do divide up the roles in a way that is acceptable to everyone, 
	and that each implements her part in the plan.
Then each conceives of her own and the others' actions as part of single plan directed to achieving a single outcome.

In the above example of agent-neutral planning,
	the housemates plan together and agree on a common plan.
	Planning together is plausibly an activity which involves shared agency.
	Note, however, that an individual can construct an agent-neutral plan by herself, even if its eventual execution will involve others.
	In fact, two or more individuals who are assigned a task might each individually engage in agent-neutral planning in parallel.
	The task demands and their planning strategies may conspire to ensure that they each come up with the same agent-neutral plan.
	The task demands and manifest properties of the agents, such as their distribution in space, may also ensure that each agent also assigns the same roles to the same individuals.
(Strictly speaking it is not necessary for the plans and role assignments to be identical; it is enough if the resulting agent-specifying plans are, in a special sense, compatible.%
\footnote{
\label{fn:df_compatible}
Suppose that, for some outcome, two or more agents each have a plan for the realisation of that outcome. 
(These plans may, but need not, specify roles for all of the agents; but the plans must be agent-specific, not agent-neutral.)
By saying that these plans are \emph{compatible} we mean that:
(i) 
no agent would normally be prevented from performing the role she is assigned in her own plan by other agents performing the roles they are assigned in their plans;
and
(ii)
if all facts about which agents have which roles in which plans were common knowledge to the agents,
this would not affect the rationality of their each acting on the intention that they realise the outcome by performing the role she is assigned in her own plan.
%	each agent could rationally act on the intention that they realise the outcome by performing the role she is assigned in her own plan,
%	and she could do so even if all facts about which agents are performing which roles in which plans were common knowledge to the agents.
To illustrate, suppose that our task is to press a button simultaneously. 
If your plan is that each of us will to start to move in exactly 60 seconds and press the button 5 seconds later whereas my plan specifies that you are the leader and we are each to press the button 5 seconds after you start moving, then our plans are compatible.
})
%
Finally, each agent may know enough about herself and the others to be able to determine, without communicating, whether the plan and role assignments will be acceptable to everyone. 
	And all of this---that they engage in parallel, agent-neutral planning resulting in identical (or compatible) plans and role assignments, which are acceptable to all---may be common knowledge to the agents.
	So it is possible, in principle at least, 
	that several agents might each individually engage in agent-neutral planning and rationally perform their part in the resulting plan, knowing that the others will do likewise.
	Parallel, agent-neutral planning can rationally result in coordinated action without presupposing shared agency.%
\footnote{
Note that we are claiming only that shared agency is not presupposed.
Our view is consistent with (but does not depend on) the claim
that
	if some agents each engage in parallel, agent-neutral planning and then rationally perform their part in the resulting plan,
	the upshot would be an exercise of shared agency.
}


Different perspectives are involved in
	forming a plan for  your own actions  only
and 
	forming a plan for  your own and others actions.
%\footnote{
%Perhaps it is impossible, strictly speaking, to plan others' actions. 
%In speaking loosely about planning your own and others actions, we mean the activity of constructing an agent-neutral plan and assigning roles as described above.
%}
  Of course, in planning your only your own actions, it may be necessary to consider how others' actions will affect, and be affected by, your won.
But in this case, the others' actions feature in your planning as constraints and opportunities.
By contrast, forming a plan for your own and others' actions 
	involves conceiving of your own and the others' actions as elements in a single plan directed to a single outcome.


Now that we have some background on agent-neutral plans,
let us introduce a third and final case.
This case needs to be as similar as possible to Beatrice and Baldric's while involving shared agency.

Caitlin and Ciaran start in the same situation as Beatrice and Baldric. 
Each is tasked with making the cross hit the red square (J$_1$). 
Once again each cares only about her own success at the outset. 
%---following version is for shortening, where the above background is skipped
%Caitlin takes the view that the best way for her to succeed is to engage in a kind of agent-neutral planning.
%A planning process is \emph{agent-neutral} when it does not involve identifying any particular agents.
%This sort of planning is quite common; for example, some housemates, having decided to go camping, might sit down to plan what needs doing without yet assigning roles to particular individuals.
%Similarly, Caitlin, knowing that she and Ciaran have the same task,
%plans how two agents in their situation could J$_1$.  
Caitlin, knowing that she and Ciaran have the same task, takes the view that the best way for her to succeed is plan how two agents in their situation could J$_1$.  
So Caitlin ends up with an agent-neutral plan.
She next assigns one role in the plan to herself and the other to Ciaran.
At this point Caitlin considers whether she would be prepared to go along with the plan given her intentions, preferences and values, and she also considers whether Ciaran would be prepared to go along with it too. 
In this case it happens that both would be prepared to go along with the plan.
Caitlin then knowledgeably predicts that Ciaran, who has similar planning abilities and has been approaching their task in a similar way, will have made an identical or compatible%
\footnote{
The notion of compatible is defined in footnote \vref{fn:df_compatible}.
}
%
 plan which, like hers, is acceptable to both of them.
Finally, Caitlin attempts to carry out her part in her plan; and Ciaran does likewise.

In short, then, Caitlin and Ciaran are like Beatrice and Baldric in nearly every respect. 
Each pair has a Bratman intention that they J$_1$,%
\footnote{
As stipulated above (see page \pageref*{df:bratman_intention}), 
two agents have a \emph{Bratman intention} that they J just if they meet conditions (1)--(3) on page \pageref{quote:bratman_account}.
}
%
 each pair acts on this intention and each pair ends up performing the same sequence of actions.
The difference is just that Beatrice and Baldric make no use of the fact that they are performing the same task.
	This fact does not show up in their planning. 
	Rather, each plans her own actions only and treats the other's actions as constraints to work around or opportunities to exploit.
By contrast, Caitlin and Ciaran embrace the fact that they are performing the same task. 
They may not like having to act together; in fact each may far prefer to act alone were that possible.
And, like Beatrice and Baldric, they are unconcerned with each other's success except insofar as their own success depends on it.
But Caitlin and Ciaran nevertheless make use of the fact that they have to perform a single task together by each constructing a single plan covering both of their actions and then carrying out their parts in these plans.
So why does Caitlin and Ciaran's case, but not Beatrice and Baldric's, plausibly involve shared agency?
The reason is  this:
	 at some stage of the planning which rationally guides and coordinates their actions,
	Caitlin and Ciaran each conceive of their actions as part of a single plan directed to achieving a single outcome.


Given the way Beatrice and Baldric's case contrasts with Caitlin and Ciaran's,
we conclude that Beatrice and Baldric do not have a shared intention.
But, by construction, 
 Beatrice and Baldric do meet Bratman's conditions for shared intention, and they do act appropriately on the corresponding intentions and knowledge. 
 So Beatrice and Baldric's case  is a counterexample to sufficiency of Bratman's conditions for shared intention (see (1)--(3) on page \pageref{quote:bratman_account}).


In describing Caitlin and Ciaran we have not introduced contralateral commitments or other elements foreign to Bratman's account of shared agency. 
Our counterexample draws on the same planning resources Bratman's account draws on, with just one addition.
The addition is the idea that agents can conceive, or fail to conceive, of their actions as part of a single plan. 
In at least some cases, shared agency differs from individual agency in part because shared agency involves changing perspective so that one’s own and other’s actions are conceived of as parts of a single plan.

Given that Beatrice and Baldric's case is a counterexample,
there are also many further counterexamples involving mundane activities and less elaborate props.
Here, for instance, is how Beatrice and Baldric walk together.
They are firmly tied at the ankle, and neither is strong enough to move without the other.
Beatrice needs to get to the corner, and so does Baldric.
Further, they have a Bratman intention that they walk to the corner, and they act on this intention in walking to the corner.
As before, each plans her own walking in the light of her predictions about how the other will walk, treating the other's actions as constraints and opportunities.
This is not a case of shared agency.
The reason is not that Beatrice and Baldric are tied together against their will; 
after all, many cases of shared agency are involuntary in this sense.
The reason their walking does not involve shared agency 
is rather that each conceives of the others' intentions and actions only as constraints and opportunities.
They fail to make any use of the fact that they intend the same outcome,
 and so they fail see their actions as part of a single plan.

Beatrice and Baldric's attitude may seem unnatural. 
Perhaps humans in their situation would not normally fail to exploit the fact that they have the same task in the way that these agents do.
This may be why Bratman's conditions appear, misleadingly, to be sufficient for shared intention.



\section{How to Fix Bratman's Account}
\label{sec:fix}

How could Bratman's account be revised to avoid the counterexample?
We might try strengthening Bratman's requirement about meshing subplans of intentions.
This requirement is that, concerning our shared intention that we J, we each intend that we J in accordance with our intentions that we J and meshing subplans of them (see (2) on page \pageref{quote:bratman_account}).
The case of Beatrice and Baldric suggests that this requirement is not strong enough.
And the case of Caitlin and Ciaran suggests that we might try strengthening it by appealing in some way to the idea that each agent can have a plan specifying roles for all of the agents.

One way to strengthen Bratman's account would be by adding a condition to his three.
An alternative, which we prefer, is to introduce a new restriction on how agents who have a shared intention may conceive of J in intending that they J.
As we have seen, Bratman's account requires only that, in some cases, the agents conceive of J in a way that is neutral with respect to shared intentionality.
We shall suggest a further requirement that is compatible with this one.

We stipulate that an activity is \emph{distributed} just if: 
\begin{enumerate}[label=\emph{\alph*})]
\item 	there is a single outcome, G,
	and two or more agents each have a plan for G which specifies a role for all of the agents;	
\item 	these plans are identical or compatible,%
\footnote{
The notion of compatibility is defined in footnote \vref{fn:df_compatible}.
}
%
		and they are each acceptable to all; 
\item	each agent performs the roles assigned to her in her own plan;
\item	the activity consists in nothing other than the agents performing these roles;
	and
\item	by performing these roles, the agents G.

\end{enumerate}
%
In specifying the class of distributed activities we have not made direct appeal to shared agency. 
Nor is any covert appeal required, as our earlier discussion should  make clear (see Section \vref{sec:distributed_plan}).
To use Bratman's term, distributed activities can be conceived of in ways that are neutral with respect to shared intentionality (see Section \vref{sec:netural_wrt_shared_intentionality}).
So we can appeal  to the notion of a distributed activity in explicating shared intention without circularity.

Many ordinary interactions involve distributed activities.
Two housemates have agreed to make a pizza. 
Each start with an idea of what needs doing.
One starts by preparing the dough, so the other peels, washes and chops the vegetables.
As the activity unfolds in this way, each gradually adds details to her plan and assigns roles. 

Distributed activities can be simple.
In walking to a metro station together,
two friends may each form and act on a plan that specifies some details of both of their movements.
This is enough for their activity to be distributed in the above sense.

When two or more agents each intend that they do something, 
they may conceive of the intended activity as a distributed activity.
Where this happens, part of what the agents intend is, in effect, that they will each, at some stage of their planning, construe their own actions as part of a single plan which also involves the others' plans.

We can exploit this possibility %of having intentions about distributed activities 
to strengthen Bratman's conditions for shared intention.
We leave the conditions for shared intention exactly as Bratman specifies them (see (1)--(3) on page \pageref{quote:bratman_account})
but add the further requirement that the agents must, in intending that they J, each conceive of J as a distributed activity.

Thus revised, the account provides conditions that are met in the case of Caitlin and Ciaran but not Beatrice and Baldric, as required.
We have not shown, of course, that this modification to Bratman's account will enable it to avoid any other counterexamples.
Perhaps further requirements are necessary.
Or perhaps it is impossible to give informative sufficient conditions for shared intention.
Our aims here were only 
		to show that Bratman's account faces counterexamples,
	and 
		to suggest a way of revising it without abandoning core features of Bratman's approach to shared intention.
		
%
%
%Consider the view that these are sufficient condition for us to have a shared intention that we J:
%%
%\begin{enumerate}
%\label{revised_account}
%\item (a) I intend that we J and (b) you intend that we J.
%% --- Bratman's condition
%% \item I intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb.
%\item I intend that we J by way of our each having and partially implementing on a plan for J-ing which specifies roles for both of us, where these plans are compatible%
%\footnote{
%The notion of compatibility is defined in footnote \vref{fn:df_compatible}.
%}
%%
%and each acceptable to both of us; and you intend likewise.
%\item 1 and 2 are common knowledge between us.
%\end{enumerate}
%%
%If the new second requirement, (2), is met then so is Bratman's.
%(This is because our having compatible plans which specify a role for both us ensures that our subplans mesh given how 
%
%Requirements (1), (2) and (4) are Bratman's; the others are new.
%This view would avoid the counterexamples,
%but at a heavy cost.
%For these conditions are only met once agents have not only done some planning but also started to act.
%While the aim is not to provide necessary conditions for shared intention, we should aim to illuminate some central cases.
%We can do better than this.
%
%For a different approach, 
%
%Consider two agents who meet Bratman's sufficient conditions for having a shared intention that they J, 
%	(1)--(3) on page ***,
%	and who each conceive of J as a distributed activity.
%	So part of what they intend when they intend that they, the two agents, J is that they perform a distributed activity.
%	For example, they might intend that they perform the distributed activity of making the cross hit the red square.
%	


%	
%	
%
%there are several agents 
%
%  which meet these conditions concerning some outcome, G:
%%
%\begin{enumerate}
%\item Each agent has a plan for how they, the agents, can G.  (This can be an agent-neutral plan plus an assignment of roles.)
%\item Each agent's plan is acceptable to every agent (it does not conflict with any individual's preferences, abilities or intentions).
%\item The agents' plans are compatible.
%\item Each agent performs the roles assigned to her in her own plan, and the agents thereby G. 
%\end{enumerate}
%%
%
%
%***Can now get rid of the meshing subplans clause,
%and probably the common knowledge clause too.
%

%
%*** each thinks what \emph{we} should do.  The fact that they have the same task shows up in their planning.
%
%
%***Simple point: the fact that we have the same task should play a role in our planning.  It is something that we should embrace, and perhaps even want.  (Be careful: two people might intentionally act together although each would prefer to act alone, and would seize the opportunity to do so.)  Cordula's idea (modified) was that we aim for shared success (it's not a question of whether I care about \\emph{your} success, but I must care about \emph{our} success).


\section{Conclusion}
We started with some contrasts between shared agency and parallel but merely individual agency.
These provide one intuitive fix on the notion of shared intention: 
 shared intention, whatever it is, is what distinguishes thinks like two friends walking somewhere together from things like two strangers who walk the same route side by side.
By introducing new contrast cases,
we provided a counterexample to the view that a certain interlocking structure of intention and knowledge is sufficient for shared intention. 
Two agents might meet conditions
Bratman offers as sufficient for shared intention---that is, it might be common knowledge to them that they might each intend that they J and intend that they J in accordance with and because of these intentions and meshing subplans of them---even though they lack a shared intention.

Bratman's conditions are not sufficient for shared intention because they leave open the possibility that two or more agents could meet his conditions while each failing to make use of the fact that there is a single outcome to which all of their actions are directed.
When this happens, each agent conceives of the others' actions merely as constraints and opportunities.
Bratman's conditions may initially appear be sufficient for shared intention because it may be natural that when for agents meet his conditions to make use of the fact that there is a single outcome to which all of their actions are directed and  to conceive of all their actions as parts of a single plan. 
Our counterexample, the case of Beatrice and Baldric, shows that although this may be natural, it not necessary.

We identified a way to revise Bratman's account so as to avoid this sort of counterexample without departing substantially from his planning approach.
The remedy we propose hinges on the possibility that an agent, in planning for an outcome, can sometimes plan both her own and other agent's actions.
Further, in the right circumstances,
two or more agents with the same task
	can each individually specify roles for all of the agents in planning for that task.
Even where such planning occurs in parallel (and so need not involve no shared agency),  it can sometimes rationally result in coordinated action.
These reflections indicate that, in giving sufficient conditions for shared intention, we can appeal to the possibility that, in acting on a shared intention, each agent has a plan for all of their actions.
This enables us to capture the idea that agents acting on a shared intention conceive of all of their actions as part of a single plan.
%One way to capture this idea, 
%put roughly and without essential qualifications (see Section \vref{sec:fix}),
% is by appeal to the idea that each agent has a plan for all of their actions.

No amount of forming intentions about other's intentions and acting on knowledge of such intentions is sufficient, all by itself, for shared intention.  
In many, perhaps all cases, shared agency requires changing perspective 
to conceive of one's own and other's intentions and actions as part of a single plan.


\bibliography{$HOME/endnote/phd_biblio}



\end{document}