 %!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\def \papersize {a4paper}

\documentclass[12pt,\papersize]{extarticle}
% extarticle is like article but can handle 8pt, 9pt, 10pt, 11pt, 12pt, 14pt, 17pt, and 20pt text

\def \ititle {Shared Agency Involves Changing Perspective: A Counterexample to Bratman}
\def \isubtitle {}
\def \iauthor {Stephen A.\ Butterfill}
\def \iemail{s.butterfill@warwick.ac.uk}
%\date{}

\input{$HOME/Documents/submissions/preamble_steve_paper3}
%\author{}
%\date{}


%avoid overhang
\tolerance=5000





%\setromanfont[Mapping=tex-text]{Sabon LT Std} 

\begin{document}

\setlength\footnotesep{1em}

%screws up word count for some reason:
\bibliographystyle{$HOME/Documents/submissions/mynewapa} %apalike

\maketitle
%\tableofcontents
\title{}

\begin{abstract}
\noindent
The leading, best developed account of shared agency, Michael Bratman's, hinges on the claim that, roughly,
we have a shared intention if we each intend that we J and also that we J by way of these intentions and meshing subplans of them where this is all common knowledge.
This paper provides a counterexample to the sufficiency of this condition for shared intention.
The counterexample arises because it is possible for agents to meet the condition, and to act rationally on the specified  intentions and knowledge,
 while conceiving of each other's actions only as constraints to work around and opportunities to exploit.
Shared agency requires more than this, or so we argue.
We also suggest a way of revising Bratman's account  that is consistent with his general approach.
In some or all cases, shared agency differs from individual agency in part because shared agency involves 
changing perspective 
to conceive of your own and another's actions  as parts of a single plan.

\end{abstract}

\section{Shared Intention}
Why, if at all, is a notion of shared intention needed? 
This question is standardly answered by appeal to contrast cases \citep[compare][p.\ 150]{Bratman:2009lv}.
Thus \citet{gilbert_walking_1990} contrasts friends intentionally walking together with two people who happen to be walking side by side. 
And \citet{Searle:1990em} contrasts park visitors who  simultaneously run to a central shelter in performing a dance with park visitors who likewise run to the central shelter but only because of an impending storm.
%And Bratman (\citeyear{Bratman:1992mi}, p.\ 333; \citeyear{Bratman:1993je}, p.\ 103--4) contrasts two people who cooperatively plan to go to New York  together with two people who each have an intention that they go to New York together by way of coercing the other into travelling with her.
These and other contrast cases invite the question, 
How do cases involving shared agency differ from cases involving only  parallel individual agency? 

The first contrast case, Gilbert's, shows that the difference can’t be just  a matter of coordination because people who merely happen to be walking side by side each other also need to coordinate their actions in order to avoid colliding.  
Note also that in both cases each individual's walking is intentional, so our intentionally walking together cannot be  only a matter of our each intentionally walking.
The second contrast case, Searle's, shows that the difference can’t just be that the resulting actions have a common effect because merely parallel actions can have common effects too.%
\footnote{
This use of contrast cases resembles \citet{Pears:1971fk}: he uses contrast cases to argue that whether something is an ordinary, individual action depends on its antecedents. 
} 
Perhaps, then, a notion of shared intention is needed to distinguish the two cases.  
Perhaps it is our acting on a shared intention that we walk together which distinguishes us from two strangers who happen to be walking side by side.%
\footnote{
Many philosophers hold that a notion of shared intention is useful for understanding acting together. 
Compare \citet[p.\ 5]{Gilbert:2006wr}: `I take a collective action to involve a collective intention.'  See also  
	\citet[p.\ 381]{Carpenter:2009wq}, 
	\citet[p.\ 369]{Call:2009fk}, 
	\citet{Kutz:2000si}, 
	\citet[p.\ 117]{rakoczy_pretend_2006} and 
	\citet{Tollefsen:2005vh}.
	}
	

But what could shared intention be?
In an influential series of papers,\footnote{ 
See \citet{Bratman:1992mi,Bratman:1993je,Bratman:1999fr,Bratman:2009lv}.
For influences beyond philosophy, see e.g.\ \citet{Tomasello:2005wx} and \citet{Knoblich:2008hy}. 
}
Bratman claims that the following are collectively sufficient\footnotemark \ conditions for you and I to have a shared intention that we J:
%
\footnotetext{
In \citet{Bratman:1992mi}, the following were offered as jointly sufficient \textit{and individually necessary} conditions; the retreat to sufficient conditions occurs in \citet[][pp.\ 143-4]{Bratman:1999fr} where he notes that `for all that I have said, shared intention might be multiply realizable.'
} 
%
\begin{quote}
\label{quote:bratman_account}
`1. (a) I intend that we J and (b) you intend that we J
 
`2. I intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb
 
`3. 1 and 2 are common knowledge between us' \citep[][p.\ View 4]{Bratman:1993je}.
\end{quote}
%
In this paper we give a counterexample to Bratman's  claim that the above conditions, (1)--(3), are collectively sufficient conditions for shared intention. 
We shall also suggest a revision to avoid the counterexample.
%Apart from improving our understanding of what shared intention is, this will also bear on the kinds of planning that are involved in acting together.


Before going further we must distinguish two versions of the claim that (1)--(3) are collectively sufficient for shared intention.
The \emph{weak claim} is that there is some J such that these conditions are sufficient for you and I to intend that we J.
The \emph{strong claim\label{strong_claim}} is that for any J, these conditions are sufficient for you and I to intend that we J. 
Our initial counterexample will be directed to the strong claim.
However, after we have constructed this counterexample it will become clear that there are also counterexamples involving a wide range mundane activities to which Bratman explicitly takes his account to apply.
Even walking together%
	\footnote{
	See \citet[p.\ 150]{Bratman:2009lv}.
	}
	%
and painting a house together%
	\footnote{
	See \citet[p.\ 331]{Bratman:1992mi}.
	}
	%
can serve as counterexamples.
%One of Bratman's aims is to show that an account of shared intention should be \emph{conceptually conservative}.
%He aims, that is, to show that  it is possible to give an account of shared intention using only concepts that `are available within the theory of individual planning agency' \citep[p.\ 163]{Bratman:2009lv}.  
%Achieving this aim would require the strong claim, and it is to the strong claim that our counterexample is directed.%
%\footnote{
%We do not aim to attack the claim that it is possible to give a conceptually conservative account of shared intention. 
%The point we are making here is just that if an account
%}
 



To show that  Bratman's conditions, (1)--(3), are not in fact sufficient for shared intention we need 
a case where the conditions are met and we lack a shared intention.
But how could we determine that we lack a shared intention?
As already mentioned, the notion of shared intention is supposed to 
	make it possible to 
	characterise systematically a difference between 
		cases involving shared agency (such as our walking together)
		and
		cases involving only parallel  individual agency (such as two strangers who happen to be walking the same route side-by-side). 
Suppose, then, that we had a trio of cases, A, B and C, each involving two agents.
Suppose, further, that A and B involved parallel agency only, whereas C involved shared agency.
Then we could be sure that A and B do not involve shared intention.
Now suppose 
	that, for some J, Bratman's conditions, (1)--(3) above, were met in cases B and C alike.
Suppose also that in each case, B and C, the structure of intention and knowledge mention specified by Bratman's conditions 
 played an appropriate role in guiding the agents' actions,
	and that in each case the agents did thereby  successfully J.
Finally, suppose that cases A, B and C are as similar as possible except for the differences mentioned.
Then case B would be our counterexample:
Bratman's conditions are met but there is no shared intention.
This is how our counterexample will work.
(Strictly speaking, case A is not necessary; but including it simplifies exposition.)

Several preliminaries are necessary for the construction of our counterexample. 
These preliminaries might easily give the impression that our counterexample depends on an artificial settings. 
However, having introduced the primary counterexample in an artificial setting, we will  go on to show that counterexamples can also be constructed for mundane activities including walking together.


%%
%%two agents who intentionally J together (e.g.\ walk  together) and two agents who also  J together but do not do so intentionally (e.g.\ they happen to be walking side by side). 
%%
%This has two consequences.
%First, it should be impossible to construct a pair cases, A and B,
%meeting these criteria:
%	(i) in each case, two agents act intentionally;
%	(ii) in the first case, the two agents do not meet conditions (1)--(3) above;
%	(iii) in the second case, the agents do meet conditions (1)--(3) above;
%	(iv) the cases, A and B, are otherwise as similar as possible;
%	and 
%	(v) neither A nor B exemplifies the sort of shared agency gestured at by appeal to the contrast cases we started with.
%Second, it should also be impossible to construct a pair cases, B and C,
%meeting these criteria:
%	(i$^\prime$) in each case, two agents act intentionally;
%	(ii$^\prime$) in each case, the agents do meet conditions (1)--(3) above;
%	and 
%	(iii$^\prime$) C but not B exemplifies the sort of shared agency gestured at by appeal to the contrast cases we started with.
%
%
%
%
%Let's stipulate that two cases \emph{differ with respect to shared agency} when they differ in the way exemplified by the contrast between our intentionally walking together and our merely walking in parallel.
%Suppose, then, that we construct three cases, A, B and C.
%The three cases are as similar as possible, except that the above conditions are met in cases B and C but not in case A.
%Now if Bratman's conditions are sufficient for shared intention, A should differ from B with respect to shared agency, and B should not differ from C with respect to shared agency.
%But suppose that in fact A does not differ from B with respect to shared agency, whereas B does so differ from C.%
%\footnote{
%Strictly speaking only A and B are needed for the counterexample, of course.
%But including C makes it clearer that A and B do not differ with respect to shared agency.
%}
%Then we can conclude that Bratman's conditions are not sufficient for shared intention.
%This is how our counterexample will work.


\section{Neutral with Respect to Shared Intentionality}
\label{sec:netural_wrt_shared_intentionality}
Before we can introduce the counterexample 
we need to highlight a feature of Bratman's account 
	which we shall be exploiting.

Consider the contents of the intentions concerning our J-ing in the above clauses, (1)--(3). 
What sort of activity can you intend when you intend that we J?
We cannot restrict possible values of J to activities which involve shared agency.
In imposing any such restriction we would be assuming the very notion that an account of shared intention is supposed to illuminate. 
%
%As we have stressed, 
%	make it possible to 
%	characterise systematically a difference between 
%		cases involving shared agency (such as our walking together)
%		and
%		cases involving parallel agency only (such as two strangers who happen to be walking the same route side-by-side). 
%We must therefore avoid tacitly appealing to this distinction by  restricting possible values of J to those which involve shared agency. 
Rather the above  conditions, (1)--(3), must be sufficient for shared intention even for some values of J which are `neutral with respect to shared intentionality'.%
\footnote{
 \citet[p.\ 147]{Bratman:1999fr}.
 This refines Bratman's earlier view that some admissable values of J are cooperatively neutral 
 	where an  act-type is \emph{cooperatively neutral} just if `joint performance of an act of that type may be cooperative, but it need not be' \citep[p.\ 330]{Bratman:1992mi}. 
}

To illustrate, first consider Bratman's `mafia case' where two people go to New York together by virtue of one of them forcing the other into a car and driving off \citep[p.\ 333]{Bratman:1992mi}. 
Next consider a paradigm case of where two friends cooperatively go to New York together.
It's possible to conceive of an act type of going to New York together which is broad enough that both cases, the coercive and the cooperative, fall under it.
And it's even possible to desire or intend to be involved in an act of this type.
A gangster might intend to go to New York with someone without having yet decided whether they will do this cooperatively or coercively.

A consequence is that, in the right situations, one of us can rationally intend that we J, and can intend this unilaterally, that is without depending on anyone else intending that we J. 
Suppose you know that I am going to New York via a certain route at a particular time, 
and that I will do this regardless of what you do.
Suppose also that you can rationally intend that you go to New York in the same manner,
and that you know that if you act on this intention the upshot will be that we will go to New York together (although I may not know that we are going together---perhaps you will conceal your presence from me).
Then you can rationally intend that we go to New York together, 
	and you can intend this irrespective of whether I have any corresponding intention---providing, of course, that in so intending you are conceiving of our going to New York together in a way that is neutral with respect to shared intentionality.
What follows depends on the premise that this is indeed possible.%
\footnote{ 
\citet{Bratman:1999fr}  defends this claim at length. 
Note also that this claim must be true if Bratman's account of shared intention is to provide an informative and systematic distinction between the contrast cases mentioned at the start.
}


\section{Unshared Intentions}
\label{sec:unshared_intentions}
As a further preliminary we need to introduce a definition.
Let us stipulate that we have an \emph{unshared intention} that we <J$_1$, J$_2$> where J$_1$$\neq$J$_2$ just if:
%
\begin{quote}
\label{df:unshared_intention}
1$^\prime$. (a) I intend that we J$_1$ and (b) you intend that we J$_2$
 
2$^\prime$. I intend that we J$_1$ in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J$_2$ in accordance with and because of la, lb, and meshing subplans of la and lb
 
3$^\prime$. 1 and 2 are common knowledge between us.
\end{quote}
In defining unshared intention we have used conditions exactly like Bratman's sufficient conditions for shared intention except that Bratman's conditions have J$_1$$=$J$_2$.
At this point it might be natural for readers to suppose that agents could not have unshared intentions,
or at least that they could not do so without irrationality.
In this section we describe a possible situation in which two agents  have an unshared intention without irrationality, deception or even ignorance.
This possible situation is not the promised counterexample, but it does form the basis for it.

Let us first introduce the activity we shall focus on.
Ayesha and Ahmed  are playing a simple video game which involves moving a cross around a two-dimensional space littered with barriers.
Ayesha can only accelerate the cross backwards or forwards,
while Ahmed can only accelerate it left or right. 
The cross moves around and interacts with the barriers in ways both players can predict.
The players are given tasks independently. 
These tasks always involve making the cross hit a target within two minutes of starting. 
A player succeeds when the cross hits her target, regardless of what happens to the cross afterwards.  
(It may go on to hit another target.)
In this case, 
	Ayesha's task is to make the cross hit the red square
	while
	Ahmed's task is make the cross hit the blue circle. 
In general it is possible that either or both will succeed, or that they will both fail.
Each movement carries a small cost to the player who moves, so that Ayesha and Ahmed each attempt to minimize how much he or she moves the cross consistently with completing his or her task.
At the outset, 
Ayesha and Ahmed are each neutral on whether the other succeeds or fails.
They are not opponents and do not seek to undermine each other's efforts, but each is entirely concerned  with his or her own task.
All of this is common knowledge for Ayesha and Ahmed.
They both know who has which task, what constraints they face and what their motives are.

Consider the possibility of one player intending, unilaterally, that the two players do something.
Suppose that one of the players---Ayesha, say---can knowledgeably predict that if she performs a certain sequence of actions, <a$_1$, a$_2$, ...\ a$_n$>, then Ahmed will simultaneously perform certain other actions, <b$_1$, b$_2$, ...\ b$_n$>,
 and the upshot will be that the cross hits the red square.
Were this to happen, it would be true that Ayesha and Ahmed made the cross hit the red square.
Suppose, further, that Ayesha can intend to perform those actions <a$_1$, a$_2$, ...\ a$_n$>.
Then Ayesha can intend, unilaterally, that they, Ayesha and Ahmed, make the cross hit the red square.
(This intention concerns an action conceived of as neutral with respect to shared intentionality; see  Section \vref{sec:netural_wrt_shared_intentionality}).

Unshared intentions require a kind of symmetry.
Let us suppose that the above sequences of actions,
	Ayesha's <a$_1$, a$_2$, ...\ a$_n$>  and 
	Ahmed's <b$_1$, b$_2$, ...\ b$_n$>,
will also result in the cross hitting the blue circle. 
(Since the cross has momentum, we can suppose that it will hit both the red square and the blue circle at some time after these action sequences have been performed.)
Then by the reasoning just offered, Ahmed could intend that they, Ayesha and Ahmed make the cross hit the blue circle.
So Ayesha and Ahmed could meet the first condition, (1), for having an unshared intention.

What about the second condition, (2)?
Suppose that Ayesha knows two further things.
First, that Ahmed intends that they, Ayesha and Ahmed, make the cross hit the blue circle.
Second, that in acting on his intention Ahmed will perform actions <b$_1$, b$_2$, ...\ b$_n$>.
Then Ayesha can intend that they, Ayesha and Ahmed, make the cross hit the red square in accordance with and because of her intention that they make the cross hit the red square and in accordance with and because of Ahmed's intention that they make the cross hit the blue circle.

This is not quite enough to meet the second condition, (2), because there is also a requirement about meshing subplans. 
To apply this requirement we need to generalise Bratman's definition of meshing:
\begin{quote}
`our individual subplans concerning our J-ing \emph{mesh} just in case there is some way we could J that would not violate either of our subplans but would, rather, involve the successful execution of those subplans' \citep[p.\ 106]{Bratman:1993je}.
\end{quote}
A natural generalisation is this:
\begin{quote}
our individual subplans concerning our <J$_1$, J$_2$>-ing \emph{mesh} just in case there is some way I could J$_1$ and you could J$_2$ that would not violate either of our subplans but would, rather, involve the successful execution of those subplans. 
\end{quote}
%
To illustrate, 
Ayesha's and Ahmed's subplans would fail to mesh if, in intending that they make the cross hit the red square,  Ayesha's plans had included pushing Ahmed out of the way and seizing his controls. 
They would also fail to mesh if Ayesha were planning to trick Ahmed into a situation where he would be unable to perform the actions he had been planning.
But in the case we have been describing, the agents' subplans mesh perfectly.
Each agent's subplans involve manipulating his or her own controls,
and the successes each seeks in doing this depends on the other successfully carrying out their subplans.
So Ayesha can rationally intend that they, Ayesha and Ahmed, make the cross hit the red square in accordance with and because of their intentions and meshing subplans of them.
And Ahmed likewise for making the cross hit the blue circle.

The only outstanding requirement for Ayesha and Ahmed to have an unshared intention is that their various intentions are common knowledge. 
Assuming common knowledge is possible concerning Bratman's conditions (1)--(2) on page \pageref{quote:bratman_account}, 
it is likewise possible for the corresponding conditions on unshared intention, (1$^\prime$)--(2$^\prime$) on page  \pageref{df:unshared_intention}.
So Ayesha and Ahmed can have an unshared intention that they <J$_1$, J$_2$> where J$_1$ is Ayesha and Ahmed's making the cross hit the red square and J$_2$ is their making the cross hit the blue circle.

So far we have shown that it is possible for two agents to have an unshared intention without irrationality, deception or ignorance.
Of course unshared intentions may be rare. 
But what matters for our counterexample is just that they are possible. 



%\section{Pure Unshared Intentions}
%We stipulate that an unshared intention is \emph{pure} if having that unshared intention does not involve having any shared intention. 
%It may be tempting to assume that all unshared intentions are pure. 
%This is not obvious, however, and may not even be true.
%Since Bratman provides only sufficient conditions for shared intention, his account doesn't tell us that an unshared intention is not a shared intention.
%For all Bratman says, the conditions defining unshared intention might also be collectively sufficient conditions for shared intention. 
%
%But doesn't shared agency require that there be a single activity, J-ing, about which each the agent involved has an intention? 
%This, too, is not obvious.  It might reasonably be denied by those who, like Bratman, reject the Simple View according to which when an individual intentionally J-s she has an intention concerning her J-ing \citep{Bratman:1984jr}.
%So the purity of an unshared intention cannot be assumed without argument. 
%This is why we do not rely on this assumption in what follows.
%
%%
%How can we show that Ayesha and Ahmed's unshared intention that they <J$_1$, J$_2$>  is pure?
%Perhaps we can appeal to intuition.
%Ayesha sees Ahmed's actions as constraints on her own, or else as opportunities.
%She exploits Ahmed's intentions for her own ends.
%Of course the situation is reciprocal: Ahmed exploits Ayesha in equal measure.
%Each allows himself or herself to be exploited by the other because being exploited enables exploiting,
%and this is the full extent of their cooperation. 
%Perhaps it is clear enough that 
%the sort of shared agency that an account of shared intention is supposed to capture must involve more than this sort of reciprocal exploitation, where each agent sees the other's actions only as constraints or opportunities.%
%\footnote{
%Note that we are not suggesting that reciprocal exploitation is incompatible with shared agency.  
%The claim under consideration here is rather that shared agency requires more than reciprocal exploitation.
%} 
%If so, we can already claim that Ayesha and Ahmed have a pure unshared intention.
%
%
%
%
%
%We started this paper by explaining the need for a notion of shared intention by appeal to contrast cases (following Bratman). 
%It is contrast cases that are supposed give us a pre-theoretical handle on shared intention.
%So to show that Ayesha and Ahmed's unshared intention is pure, we need to construct a contrasting case.
%
%***Caitlin and Ciaran, 
%	like many people, 
% 	sometimes attribute intentions and other states to imaginary agents, and perform actions on their behalf. 
%In some cases this enables them to further their own real-world objectives (as in `Teddy wants to go to the park now').	
%Some of these imaginary agents are toys. 
%But some are imaginary aggregate agents. 
%Sometimes, that is, Caitlin and Ciaran imagine that there is an agent distinct from either of them and such that all its parts are parts of them.
%They attribute intentions and other states to this agent, and act on its behalf.
%
%
%


%
%An account of shared intention should enable us to  distinguish systematically between two types of case in which we J together, one where we intentionally J together and the other where  our J-ing together involves  merely parallel actions.
%As noted, Gilbert's example of walking together shows that merely parallel actions can be mutually responsive, as when two strangers walking side by side avoid collision thanks to meshing subplans.



\section{The Counterexample}
\label{sec:the_counterexample}

In the situation just described,
Ayesha and Ahmed are playing a game and have different tasks.
Thanks to special features of the game environment, they both succeed by acting on an unshared intention.
Now compare two further players, Beatrice and Baldric, who are playing the same game. 
Their situations, knowledge states, intentions and actions are as similar as possible to Ayesha's and Ahmed's except for one detail.
Just by chance they have been assigned identical tasks: Beatrice's task is to make the cross hit the red square and Baldric's task is the same.
So where Ayesha and Ahmed have an unshared intention that they <J$_1$, J$_2$>,
Beatrice and Baldric meet Bratman's conditions (1)--(3) for having a shared intention that they J$_1$.  
But Beatrice, in planning and acting, does not rely on the coincidence of their intentions; and nor does Baldric.
(Beatrice relies on the fact Baldric intends that they J$_1$, of course; but she does not rely on the fact that Baldric intends what she intends.) 
Let us further stipulate that, due to an artefact of the way the game is structured,
the unshared intention and the 
	Bratman intention 
	\label{df:bratman_intention}
	(as we might label the structure of intention and knowledge 
	%specified in conditions (1)--(3) on page \pageref{quote:bratman_account}
	while leaving open whether it constitutes a shared intention) result in the two pairs performing same actions in the same way.
That is, Beatrice reasons about Baldric much as Ayesha reasons about Ahmed and Beatrice and does what Ayesha does; and likewise for Baldric and Ahmed. 


We claim that Beatrice and Baldric have a shared intention that they J$_1$
only if 
Ayesha and Ahmed have a shared intention.%
\footnote{
Strictly speaking, 
	what matters for our argument is whether Ayesha and Ahmed have a shared intention \emph{in virtue of having the unshared intention that they <J$_1$, J$_2$>}.
	This is because, strictly speaking, we need to show, not that Beatrice and Baldric lack any shared intention whatsoever, but only that they lack a shared intention that they J$_1$ just in virtue of meeting Bratman's conditions (see (1)--(3) on page  \pageref{quote:bratman_account}). 
	For ease of exposition this is not made explicit in the main text.
} 
This claim follows from the similarities of the two cases.
The only difference is that Beatrice and Baldric happen to be assigned the same task, whereas Ayesha and Ahmed are not.
And neither Beatrice nor Baldric makes use of the fact that they have the same task. 
(This is not due to ignorance: it's just how they choose to approach their tasks.)
So if we consider  how  
		 Beatrice and Baldric's case
	differs from
		 that of Ayesha and Ahmed,
we can see that these differences do not plausibly amount to a difference with respect to shared agency.
Shared intention cannot feature in one case but not the other.

To show that Beatrice and Baldric do not have a shared intention it remains only to show that Ayesha and Ahmed do not have one. 
Here we must be careful.
First note that,
	 since Bratman provides only sufficient conditions for shared intention, 
	 his account doesn't tell us that an unshared intention is not a shared intention.
For all Bratman says, the conditions defining unshared intention might  be sufficient for shared intention. 

But doesn't shared intention require at least this much,
that there be a single activity  about which each the agent involved has an intention? 
This might reasonably be doubted by those who, like Bratman, reject the Simple View according to which when an individual intentionally J-s she has an intention concerning her J-ing \citep{Bratman:1984jr}.
So 
	we shall not infer  that Ayesha and Ahmed lack a shared intention
	just because (by construction) there is no J such that Ayesha and Ahmed each intend that they, Ayesha and Ahmed, J.

Can we then appeal directly to intuition to show that Ayesha and Ahmed lack a shared intention?
Ayesha sees Ahmed's actions as constraints on her own, or else as opportunities.
She exploits Ahmed's intentions for her own ends.
Of course the situation is reciprocal: Ahmed exploits Ayesha in equal measure.
Each allows himself or herself to be exploited by the other because being exploited enables exploiting,
and this is the full extent of their cooperation. 
We don't suppose that reciprocal exploitation is incompatible with shared intention. 
But Ayesha and Ahmed's interaction consists entirely in this sort reciprocal exploitation, where each agent sees the other's actions only as constraints or opportunities.
Perhaps it is clear enough that 
the sort of shared agency that an account of shared intention is supposed to capture must involve more than this. 
If so, we can already claim that Ayesha and Ahmed are not acting on a shared intention.
But philosophers' intuitions about shared agency may vary, so it would be better if we could avoid such a blunt appeal to intuition.

How else could we support the claim that Beatrice and Baldric lack a shared intention? 
As mentioned at the start, 
the contrast cases are often used to anchor intuitions in theorising about shared agency.
In the next sections we shall further support our claim by contrasting Beatrice and Baldric's case with a further case, one which is as similar as possible and which does seem to involve shared agency.
The fact that Beatrice and Baldric's case contrasts with this new case will support the claim that Beatrice and Baldric lack shared intention.
This is the proximal aim of the following sections.
A more distal aim 
is to understand why Bratman's conditions are not sufficient for shared intention
and, relatedly, to provide materials for fixing his account.



\section{Individually planning all of  the agents' actions}
\label{sec:distributed_plan}

Above we introduced what will turn out to be a counterexample to Bratman's account, the case of Beatrice and Baldric. 
But we have yet to show that this case really is a counterexample. 
For all we have said so far, a proponent of Bratman's view might insist that Beatrice and Baldric do have a shared intention that they J$_1$.
To show that they do not,
we shall contrast Beatrice and Baldric's case with a third case that is as similar as possible but does involve shared intention (see Section \vref{sec:new_contrast_case}). 
Before we can introduce this third case,
we first need some background on planning.



%---the following is a mistake (because Beatrice and Baldric do take each other's task into account.
%It is unlikely that humans would spontaneously behave as Beatrice and Baldric do.%
%\footnote{
%This is nicely illustrated in a series of experiments by Sebanz and colleagues in which subject spontaneously take another's plans into account in planning their own actions, even where doing so is manifestly not required and can impair their performance  \citep{Sebanz:2003kf,Sebanz:2005fk,tsai:2011_groop_effect}.
%}

We stipulate that a planning process, or a plan, is \emph{agent-neutral} just if it does not involve identifying any particular agents.  
This sort of planning is quite common.
For example, some housemates who have decided to make a pizza might sit down together to plan what needs doing without yet assigning roles to particular individuals. 
Someone needs to prepare the dough, 
another person should prepare toppings,
while a third might mix some salad.
In so planning, each housemate is thinking about what 
agents in their situation should do 
and not what she herself or anyone in particular will do.  
%(This is an idealisation, of course.)
At some point the housemates stop planning.
(This does not necessarily mean that they have a fully worked out plan; like any other plans, agent-neutral plans can have gaps that may need filling in later.)
They now divide up the roles.
Of course they may not find a way of dividing up roles that everyone is prepared to go along with---individuals' preferences, abilities and intentions may block the plan's adoption.
But suppose the housemates do divide up the roles in a way that is acceptable to everyone, 
	and that each implements her part in the plan.
Then each conceives of her own and the others' actions as part of single plan directed to achieving a single outcome.

In this example
	the housemates plan together and agree on a common plan.
	Planning together is plausibly an activity which involves shared agency.
	Note, however, that an individual can construct an agent-neutral plan by herself, even if its eventual execution will involve others. 
	In fact, two or more individuals who are assigned the same task and who will eventually collaborate might first each individually engage in agent-neutral planning and then individually assign roles.
	And they might do this in isolation from each other, without discussion or communication.

Where there is an outcome and some agents each individually,
without discussion or communication,
plan for the outcome 
and where each agent has a single plan specifying roles for herself and all of the other agents,
%When there is an outcome and some agents each individually,
%without any discussion or communication,
%construct an agent-neutral plan for this outcome and then assign roles to themselves and the other agents,
we shall say that these agents each \emph{individually plan all of the agents' actions}.%
\footnote{
This phrase is ambiguous (in a way we know  no way concise of avoiding).
The idea is that each agent has a single plan in which she and  all other agents are assigned roles.
There is no requirement that any agent's plan must specify all the actions which the agents between them will perform.
}
Agents can do this by first constructing agent-neutral plans and then assigning roles to themselves and the other agents.
So, to return to our earlier example, 
 if the housemates, having decided to make a pizza, had each individually devised  agent-neutral plans for the construction of the pizza and then each individually assigned roles in their own plans to themselves and the others, 
then they would have each individually planed all of the agents' actions.

For all we have said,  agent-neutral plans might not be involved in all cases where some agents each individually plan all of the agents' actions.
We invoke agent-neutral plans only as an aid to showing that the notion of some 
agents each individually planning all of the agents' actions is theoretically coherent.


But why would some agents ever each individually plan all of the agents' actions?
One might suppose that this would occur only when each agent thinks that she in charge, or is fantasizing about controlling the others.
If you know you aren't the boss and aren't going to decide what another agent will do, why  make a plan for that agent's actions as well as your own?
Why not just plan your own actions?
As we shall now explain, 
one reason why some agents might each plan all of the agents' actions 
	even when there is manifestly no leader and the agents all have the same status
	is that doing so provides one way to solve a coordination problem.
%%having agents each individually plan all of the agents' actions
%can be a way of achieving coordination even where there is no leader and the agents all have the same status.

%How could  this work?
Sometimes a plan for your own actions needs to be constrained by, and to constrain, plans for other agents' actions.
When, for example,
you and someone else are about to move a heavy table from one room to another,
there are constraints 
on relations between
	your actions
	and
	the other's actions.
These relational constraints concern timings, forces, choices of route and more besides.
Now meeting such relational constraints does not in principle require you to plan actions other than your own.
In principle,  you  might make an initial plan for your own actions only.
You could use you initial plan to make predictions about the  other's likely actions,
and then revise this initial plan for your actions in the light of these predictions about the other's actions.
This cycle of prediction and revision might be repeated.
Your plan might also involve conditional elements to accommodate unpredicted actions.
So if you plan your own actions only,
 you will need to rely on predictions about what the other will do,
 and these predictions will sometimes depend on what you plan to do.
But there is another way to proceed,
one which avoids the need for a plan that depends on predictions which in turn depend on the plan.
For you can use the same planning processes which enable you to meet constraints on relations between two of your own actions
	in order to meet constraints on relations between your own and the other's actions.
That is, you could make a single plan for both your own and the other's actions.
(You might do this by first making an agent-neutral plan and then assigning roles.)
Making a single plan for your actions and the other's actions is a way of using your planning abilities to meet relational constraints.
Of course, 
this is only rational  if you are in a position to know that your plan for the other's actions will be sufficiently similar to her own plan for her actions.
And it is unlikely that you could know this if 
	you and the other agent differed too much in expertise,
	if there were two or more equally good ways to achieve the outcome you are planning,
	or if there were two or more equally good ways to assign roles.
But suppose that you and the other person have similar expertise in moving furniture, 
	that there is obviously exactly one most optimal route,
	 and that your positions in space manifestly determine which ends of the table you should each hold and so which role each of you should play. 
Then it is likely that you could  be in a position to know,
	even without communication,
	that your plan for the other agent's actions will nearly enough coincide with her own plan for her actions.
So  making a single plan for your actions and the other's actions can be a way of 
	working out how to 
	meet constraints on the relation between your actions and hers and so of achieving coordination.  
(Note that, as already mentioned,  we don't claim that this  is the only or best way to achieve coordination.)	
And, clearly, this applies equally to the 
 other person you are lifting the table with.
 She  could work out how to coordinate her actions with yours by likewise making a single plan for her actions and yours.%
\footnote{
As an aside, we note that 
\citet{tsai:2011_groop_effect} provide evidence indicating that planning for your own and and others' actions happens not only in conscious practical reasoning but also in planning that you may not ordinarily be directly aware of.
}

The question was why some agents would each individually plan all of the agents' actions.
We have just argued that  
one reason is that
doing this would sometimes enable the agents to solve a coordination problem.
Agents who will collaborate in an activity, such as moving a table or making a pizza, 
can sometimes individually plan all of the agents' actions,
knowing that each agent's plan is sufficiently similar to the other agents' plans.
And sometimes each can rationally perform her part in her plan for all of the agent's actions, knowing that the others will do likewise.%
\footnote{
Note that this coordination is achieved without presupposing that the agents exercise shared agency.
Of course,
	if some agents individually plan all of the agents' actions
	and then each rationally performs her part in her own plan,
	the upshot might be an exercise of shared agency.
But this shared agency is a consequence of, not a requirement on, their individually planning all of the agents' actions.
}


Now recall Beatrice and Baldric (from Section \vref{sec:the_counterexample}).
We said earlier that  they each see the other's intentions and actions  as constraints to work around or opportunities to exploit only.
This is because neither agent plans all of their actions and so neither conceives of both her own and the other's actions as parts of a single plan.
In constructing a case that contrasts with Beatrice and Baldric's we shall rely on the distinction between
a plan that includes both your own and another's actions
and 
a plan that merely exploits, and is constrained by, predictions about the other's actions.
This distinction matters independently of whether two such plans would result in different actions.
It matters because 
making a plan for your own and another's actions 
	involves changing perspective.
That is, it involves conceiving of the other's actions not only as constraints and opportunities, but as on a par with your own actions insofar as your actions and their actions are elements in a single plan directed to a single outcome.

Note that our key distinction is not between merely predicting and planning another agent's actions.
Beatrice and Baldric each intend to make the cross hit the red square (that is, to J${_1}$) in accordance with, and in part because of, each other's intentions.
So when Beatrice predicts what Baldric will do, she can rely on facts about his intentions.
She may even make predictions by planning his actions,
and Baldric may likewise predict Beatrice's actions by planning them.
So we can suppose that each plans the other's actions as well as separately planning her own actions,
and that these two planning activities constrain each other.
But---and this is our key distinction---neither makes a single plan for both her own and the other's actions.



\section{Why Making One Plan or Two Matters}
The distinction between making a single plan and making two separate plans may initially seem too subtle to be useful, 
especially as 
the two separate plans may be interdependent.
Further, if two separate plans could both be executed (not just each plan but both together), then they can be combined into a single plan.
The combined plan is simply to implement each of the two formerly separate plans. 
Given these apparent grounds for scepticism,
in this section we shall explain why the distinction between making a single plan and making two separate plans matters.


***First,
the fact that two separate \emph{plans} can be combined doesn't undermine the distinction between \emph{making} a single plan and \emph{making} two separate plans.
To see why consider structure ...
[... but the really deep point is not about structure: it's about keeping plans separate meaning adopting a predictive rather than a deliberative perspective towards the actions ...]

Let us step back from shared agency  
 to consider how 
 the distinction between making a single plan and making two separate plans
  applies in cases of ordinary, individual agency.
Suppose you have to organise two events, a workshop and a wedding.
If the two events were largely independent of each other,
it would be simplest to make two separate plans for them.
But in this case there are all kinds of constraints linking the two events.
Some participants at the workshop are also wedding guests,
and it happens that many choices of transport, entertainment and catering for one event constrain possible choices for the other event.
In principle you could handle these by making two separate plans where, at any stage of your planning, making one plan involves treating the other plan as providing constraints to work around and opportunities to exploit.
But envisaging the complex interdependence of these two plans  
might reasonably incline you to make a single plan for the wedding and workshop.
Making a single plan can be simpler because it allows for structuring the requirements differently.
For instance, if you have two separate plans, you will need subplans for transport in each plan and so are forced to divide the problem of transporting participants into the problem of transporting them for the workshop and the problem of transporting them for the wedding.
Having a single plan for both events allows you to treat transport as a single problem (but it does not require you to do so, of course).
As this indicates, the distinction between making a single plan and making two separate but interdependent plans is not merely notional.
Agents can have reasons for making a single plan rather than two separate ones.

This claim needs qualification, however.
It might be held that single agents always combine all plans for their own actions ... unity of their plans ... leads to the Moran point ***

 
 
There is a familiar and  important distinction between merely predicting your own actions and planning them.%
%
\footnote{
Even Velleman, who argues that `[d]eciding on an action … is just a peculiar way of predicting it' (\citeyear[p.\ 48]{Velleman:1985mc}), can accommodate a distinction between merely predicting and planning actions.
In his terms, we might say that 
planning involves not merely predicting but also being inclined to make the prediction true \citep[see][p.\ 195]{Velleman:2000fq}.
%“How can one regard a proposition as true in such a way as to make it true? Well, when one accepts a proposition in response to its truth, one registers the influence of evidence and other reasons for belief, thereby manifesting an inclination to conform one’s acceptance to the facts. Accepting a proposition in such a way as to make it true would simply require a converse inclination, to conform the facts to one’s acceptance. And one can indeed be inclined to conform the facts to one’s acceptance, if the proposition accepted is about one’s own behavior. One need only be inclined to do what one accepts that one will do. If one has this inclination, then accepting that one will do something can be a way of making this proposition true, and it can therefore be an attempt at accepting the truth.”  (195)
}
%
It is possible and perhaps sometimes useful to make predictions about your future actions, of course.
%*nb 'your ... oneself' issue in this sentence:
But to rely on mere predictions about your own actions in planning can involve a kind of estrangement from oneself \citep[compare][]{Moran:2001hr}.



We are arguing that 
something similar applies in the case of shared agency, 
except that here the distinction between predicting and planning actions also applies to others' actions.


Applying this distinction in cases of shared agency requires additional subtlety, of course.
After all,
you cannot rationally include another's actions in a plan you know you will act on unless you could predict the other's actions.%
%
\footnote{
This statement requires qualification if, as \citet{roth_shared_agency} argues, 
another agent might act directly on your intentions. 
As the cases we are considering involve agents in symmetric roles, we can ignore that possibility here.
We acknowledge, however, that Roth's idea has inspired the less radical view defended in this section.
}
%
This fact should not obscure the distinct ways in which one conceives of another agent's actions in planning and predicting them, however.
For, as we have argued, in some cases planning another agent's actions does not require actually predicting them.
In some cases, planning another's actions does not depend on having predicted them but rather provides a ground for prediction.
This is why 
different perspectives are involved in 
	merely predicting another's actions 
	and 
	conceiving of her actions and  your own actions as elements of a single plan.
	
	
	
\section{A New Contrast Case}
\label{sec:new_contrast_case}
Now that we have some background on how some agents might each individually plan all of the agents' actions,
we can introduce a third and final case.
This case needs to be as similar as possible to Beatrice and Baldric's while involving shared agency.

Caitlin and Ciaran start in the same situation as Beatrice and Baldric. 
Each is tasked with making the cross hit the red square (J$_1$). 
Once again each cares only about her own success at the outset. 
%---following version is for shortening, where the above background is skipped
%Caitlin takes the view that the best way for her to succeed is to engage in a kind of agent-neutral planning.
%A planning process is \emph{agent-neutral} when it does not involve identifying any particular agents.
%This sort of planning is quite common; for example, some housemates, having decided to go camping, might sit down to plan what needs doing without yet assigning roles to particular individuals.
%Similarly, Caitlin, knowing that she and Ciaran have the same task,
%plans how two agents in their situation could J$_1$.  
Caitlin, knowing that she and Ciaran have the same task, takes the view that the best way for her to succeed is plan how two agents in their situation could J$_1$.  
So Caitlin ends up with an agent-neutral plan.
She next assigns one role in the plan to herself and the other to Ciaran.
At this point Caitlin considers whether she would be prepared to go along with the plan given her intentions, preferences and values, and she also considers whether Ciaran would be prepared to go along with it too. 
In this case it happens that both would be prepared to go along with the plan.
Caitlin then knowledgeably predicts that Ciaran, who has similar planning abilities and has been approaching their task in a similar way, will have made a sufficiently similar plan.
Finally, Caitlin attempts to carry out her part in her plan; and Ciaran does likewise.

In short, then, Caitlin and Ciaran are like Beatrice and Baldric in nearly every respect. 
Each pair has a Bratman intention that they J$_1$,%
\footnote{
As stipulated above (see page \pageref*{df:bratman_intention}), 
two agents have a \emph{Bratman intention} that they J just if they meet conditions (1)--(3) on page \pageref{quote:bratman_account}.
}
%
 each pair acts on this intention and each pair ends up performing the same sequence of actions.
The difference is just that Beatrice and Baldric make no use of the fact that they are performing the same task.
	This fact does not feature in their planning. 
	Rather, each plans her own actions only and treats the other's actions as constraints to work around or opportunities to exploit.
By contrast, Caitlin and Ciaran embrace the fact that they are performing the same task. 
They may not like having to act together; in fact each may far prefer to act alone were that possible.
And, like Beatrice and Baldric, they are unconcerned with each other's success except insofar as their own success depends on it.
But Caitlin and Ciaran nevertheless make use of the fact that they have to perform a single task together by each constructing a single plan including both of their actions and then carrying out their parts in these plans.
So why does Caitlin and Ciaran's case, but not Beatrice and Baldric's, plausibly involve shared agency?
The reason is  this:
	 at some stage of the planning which rationally guides and coordinates their actions,
	Caitlin and Ciaran each conceive of both of their actions as parts of a single plan directed to achieving a single outcome.


Given the way Beatrice and Baldric's case contrasts with Caitlin and Ciaran's,
we conclude that Beatrice and Baldric do not have a shared intention.
But, by construction, 
 Beatrice and Baldric do meet Bratman's conditions for shared intention, and they do act appropriately on the corresponding intentions and knowledge. 
 So Beatrice and Baldric's case  is a counterexample to sufficiency of Bratman's conditions for shared intention.


In describing Caitlin and Ciaran we have not introduced contralateral commitments or any other element foreign to Bratman's account of shared agency. 
Our counterexample draws on the same planning resources Bratman's account draws on, with just one addition.
The addition is the idea that an agent 
	might plan  her own actions only 
	or else 
	might plan both her own and others' actions.
Agents often conceive of others' actions as external to their own plans, and so as either entirely irrelevant or constraints and opportunities only.
But shared agency sometimes or always involves agents who each individually plan all of the agents' actions and so conceive of their own and the others' actions  as parts of a single plan.
In at least some cases, shared agency can be distinguished from individual agency in part because it involves changing perspective in this way.


\section{Mundane Counterexamples}
Beatrice and Baldric's case is a counterexample to the claim that for any J, Bratman's conditions, (1)--(3) on page \pageref{quote:bratman_account}, are collectively sufficient for us to have a shared intention that we J.
(We called this the `strong claim'  \vpageref{strong_claim}.)
If this counterexample were an isolated case, it might be tempting to suppose that it can provide little insight into shared agency.
%After all, the sorts of activity that Bratman focuses on are things like walking together and painting the house together.
Could proponents of Bratman's view avoid counterexamples by claiming that the his conditions are sufficient for mundane exercises of shared agency such as those involved in walking together or painting a house together?
In this section we show that they could not.
Given that Beatrice and Baldric's case is a counterexample,
there are also many further counterexamples involving mundane activities and less elaborate props.

Here is how Beatrice and Baldric walk together.
They are firmly tied at the ankle, and neither is strong enough to move without the other.
Beatrice needs to get to the corner, and so does Baldric.
Further, they have a Bratman intention that they walk to the corner, and they act on this intention in walking to the corner.
As before, each plans her own walking in the light of her predictions about how the other will walk, treating the other's actions as constraints and opportunities.
This is not a case of shared agency.
The reason is not that Beatrice and Baldric are tied together against their wills; 
after all, many cases of shared agency involve agents who are unwillingly bound to each other.
The reason their walking does not involve shared agency 
is rather that each conceives of the others' intentions and actions only as constraints and opportunities.
Beatrice has a plan for what she, Beatrice, will do to ensure that they arrive at the corner; and Baldric likewise plans what he only will do.
Each agent's plan relies on predictions about how the other will act and contains subplans which are conditional on the other's subplans and actions.
But this involves no more than the sort of interdependence that might hold between two plans directed at different ends, as in cases of unshared intention.
Beatrice and Baldric fail to make any use of the fact that they each intend the same thing, namely that they walk to the corner.


For another counterexample, consider house painting.  
First imagine that Ayesha and Ahmed are tasked with painting the outsides of adjacent houses on the same day,
which happens to be windy.
Each wants their house to be painted a single colour, and would prefer that it not be painted at all rather than that it be painted in several colours.
But because they are spraying the paint onto the houses in blustery winds, neither can avoid spraying a significant quantity of paint onto the other's house as well as their own house. 
In one respect this is fortunate. 
For they will each lose so much paint from spraying in windy weather that, normally, neither would have enough paint to cover her house.
But if they each make use of wind-borne spray from the other, both can succeed.
They therefore form an unshared intention concerning their house painting projects. 
(The notion of  unshared intention is explained in Section \vref{sec:unshared_intentions}.)
In acting on this unshared intention,
Ayesha starts painting first, using a colour which she knows is acceptable and available to Ahmed.
Ayesha and Ahmed then each plan a route around their own house which will keep them out of the path of the other painter while also saving paint by taking advantage of foreseeable  wind-borne paint from the other.

Ayesha and Ahmed's house painting involves only unshared intention and is therefore not a counterexample.
But now imagine that there is a single house which Beatrice and Baldric are each motivated to paint.
Because neither has sufficient paint for the job, they need to work together if they are to succeed.
They therefore form a Bratman intention that they paint the house.
But in acting on this intention, they plan and act much as Ayesha and Ahmed do.
Each agent plans what she will do to ensure that they paint the house, treating the other's intentions and actions as constraints and opportunities. 
Although the agents' plans and intentions interlock, 
the agents do not conceive of their actions as parts of a single plan.
From either agent's point of view, their intentions and actions are no more intimately related than are Ayesha and Ahmed's.
This is not shared agency.


Beatrice and Baldric's attitude may seem unnatural. 
Perhaps it is hard to believe that ordinary people in their situation would  fail to exploit the fact that they have the same task.
This may be why Bratman's conditions appear, misleadingly, to be sufficient for shared intention.



\section{How to Fix Bratman's Account}
\label{sec:fix}

How could Bratman's account be revised to avoid the counterexample?
We might try strengthening Bratman's requirement about meshing subplans of intentions.
This requirement is that, concerning our shared intention that we J, we each intend that we J in accordance with our intentions that we J and meshing subplans of them (see (2) on page \pageref{quote:bratman_account}).
The case of Beatrice and Baldric suggests that this requirement is not strong enough.
And the case of Caitlin and Ciaran suggests that we might try strengthening it by appealing in some way to the idea that each agent can have a plan specifying roles for all of the agents.

One way to strengthen Bratman's account would be by adding a fourth condition his three conditions.
An alternative, which we prefer, is to introduce a new restriction on how agents who have a shared intention may conceive of J in intending that they J.
As we have seen, Bratman's account requires only that, in some cases, the agents conceive of J in a way that is neutral with respect to shared intentionality (see Section \vref{sec:netural_wrt_shared_intentionality}).
We shall suggest a further requirement that is compatible with this one.

We stipulate that an activity is \emph{distributed} just if: 
\begin{enumerate}[label=\emph{\alph*})]
\item 	there is a single outcome, G,
	and two or more agents each have a plan for G which specifies  roles for all of the agents;	
\item 	these plans are identical or compatible,%
\footnote{
The notion of compatibility is defined in footnote \vref{fn:df_compatible}.
}
%
		and they are each acceptable to all; 
\item	each agent performs the roles assigned to her in her own plan;
\item	the activity consists in nothing other than the agents performing these roles;
	and
\item	by performing these roles, the agents G.

\end{enumerate}
%
In specifying the class of distributed activities we have not made direct appeal to shared agency. 
Nor is any covert appeal required, as our earlier discussion should  make clear (see Section \vref{sec:distributed_plan}).
To use Bratman's term, distributed activities can be conceived of in ways that are neutral with respect to shared intentionality (see Section \vref{sec:netural_wrt_shared_intentionality}).
So we can appeal  to the notion of a distributed activity in explicating shared intention without circularity.

Many ordinary interactions involve distributed activities.
Two housemates have agreed to make a pizza. 
Each start with an idea of what needs doing.
One starts by preparing the dough, so the other peels, washes and chops the vegetables.
As the activity unfolds in this way, each gradually adds details to her plan and assigns roles. 

Distributed activities can be simple.
In walking to a metro station together,
two friends may each make and act on a plan that specifies some details of both of their actions.
This is enough for their activity to be distributed in the above sense.

When two or more agents each intend that they do something, 
they may conceive of the intended activity as a distributed activity.
Where this happens, part of what the agents intend is, in effect, that they will each, at some stage of their planning, construe their own actions as part of a single plan which also involves the others' plans.

We can exploit this possibility %of having intentions about distributed activities 
to strengthen Bratman's conditions for shared intention.
We leave the conditions for shared intention exactly as Bratman specifies them (see (1)--(3) on page \pageref{quote:bratman_account})
but add the further requirement that the agents must, in intending that they J, each conceive of J as a distributed activity.

Thus revised, the account provides conditions that are met in the case of Caitlin and Ciaran but not Beatrice and Baldric, as required.
We have not shown, of course, that this modification to Bratman's account will enable it to avoid any other counterexamples.
Perhaps further requirements are necessary.
Or perhaps it is impossible to give informative sufficient conditions for shared intention.
Our aims here were only 
		to show that Bratman's account faces counterexamples,
	and 
		to suggest a way of revising it without abandoning core features of Bratman's approach to shared intention.
		
%
%
%Consider the view that these are sufficient condition for us to have a shared intention that we J:
%%
%\begin{enumerate}
%\label{revised_account}
%\item (a) I intend that we J and (b) you intend that we J.
%% --- Bratman's condition
%% \item I intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb.
%\item I intend that we J by way of our each having and partially implementing on a plan for J-ing which specifies roles for both of us, where these plans are compatible%
%\footnote{
%The notion of compatibility is defined in footnote \vref{fn:df_compatible}.
%}
%%
%and each acceptable to both of us; and you intend likewise.
%\item 1 and 2 are common knowledge between us.
%\end{enumerate}
%%
%If the new second requirement, (2), is met then so is Bratman's.
%(This is because our having compatible plans which specify a role for both us ensures that our subplans mesh given how 
%
%Requirements (1), (2) and (4) are Bratman's; the others are new.
%This view would avoid the counterexamples,
%but at a heavy cost.
%For these conditions are only met once agents have not only done some planning but also started to act.
%While the aim is not to provide necessary conditions for shared intention, we should aim to illuminate some central cases.
%We can do better than this.
%
%For a different approach, 
%
%Consider two agents who meet Bratman's sufficient conditions for having a shared intention that they J, 
%	(1)--(3) on page ***,
%	and who each conceive of J as a distributed activity.
%	So part of what they intend when they intend that they, the two agents, J is that they perform a distributed activity.
%	For example, they might intend that they perform the distributed activity of making the cross hit the red square.
%	


%	
%	
%
%there are several agents 
%
%  which meet these conditions concerning some outcome, G:
%%
%\begin{enumerate}
%\item Each agent has a plan for how they, the agents, can G.  (This can be an agent-neutral plan plus an assignment of roles.)
%\item Each agent's plan is acceptable to every agent (it does not conflict with any individual's preferences, abilities or intentions).
%\item The agents' plans are compatible.
%\item Each agent performs the roles assigned to her in her own plan, and the agents thereby G. 
%\end{enumerate}
%%
%
%
%***Can now get rid of the meshing subplans clause,
%and probably the common knowledge clause too.
%

%
%*** each thinks what \emph{we} should do.  The fact that they have the same task shows up in their planning.
%
%
%***Simple point: the fact that we have the same task should play a role in our planning.  It is something that we should embrace, and perhaps even want.  (Be careful: two people might intentionally act together although each would prefer to act alone, and would seize the opportunity to do so.)  Cordula's idea (modified) was that we aim for shared success (it's not a question of whether I care about \\emph{your} success, but I must care about \emph{our} success).


\section{Conclusion}
We started with some contrasts between shared agency and parallel but merely individual agency.
These provide an intuitive fix on the notion of shared intention: 
 shared intention, whatever it is, is what distinguishes thinks like two friends walking somewhere together from things like two strangers who walk the same route side by side.
By introducing new contrast cases,
we provided a counterexample to the view that a certain interlocking structure of intention and knowledge is sufficient for shared intention. 
Two agents might lack a shared intention even though they meet conditions Bratman offers as sufficient for shared intention
That is, they might lack a shared intention even though it is common knowledge to them that they  each intend that they J and intend that they J in accordance with and because of these intentions and meshing subplans of them.

Bratman's conditions are not sufficient for shared intention because they leave open the possibility that two or more agents could meet his conditions while each failing to make use of the fact that there is a single outcome to which all of their actions are directed.
When this happens, each agent conceives of the others' actions merely as constraints and opportunities.
Bratman's conditions may initially appear be sufficient for shared intention because it may be natural for agents who meet his conditions to make use of the fact that there is a single outcome to which all of their actions are directed and  to conceive of all their actions as parts of a single plan. 
Our counterexample, the case of Beatrice and Baldric, shows that although this may be natural, it not necessary.

We identified a way to revise Bratman's account so as to avoid this sort of counterexample without departing substantially from his planning approach.
The remedy we propose hinges on the possibility that an agent, in planning for an outcome, can sometimes plan both her own and other agents' actions.
Further, in the right circumstances,
two or more agents with the same task
	can each individually specify roles for all of the agents in planning for that task.
Even where such planning occurs in parallel (and so need not involve no shared agency),  it can sometimes rationally result in coordinated action.
These reflections indicate that, in giving sufficient conditions for shared intention, we can appeal to the possibility that, in acting on a shared intention, each agent has a plan for all of their actions.
This enables us to capture the idea that agents acting on a shared intention conceive of all of their actions as part of a single plan.
%One way to capture this idea, 
%put roughly and without essential qualifications (see Section \vref{sec:fix}),
% is by appeal to the idea that each agent has a plan for all of their actions.

No amount of forming intentions about other's intentions and acting on knowledge of such intentions is sufficient, all by itself, for shared intention.  
In many or all cases, shared agency requires changing perspective 
to conceive of your own and another's intentions and actions as parts of a single plan.


\bibliography{$HOME/endnote/phd_biblio}



\end{document}