 %!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\def \papersize {a4paper}

\documentclass[12pt,\papersize]{extarticle}
% extarticle is like article but can handle 8pt, 9pt, 10pt, 11pt, 12pt, 14pt, 17pt, and 20pt text

\def \ititle {Is This a Counterexample to Bratman on Shared Intention?}
\def \isubtitle {}
\def \iauthor {Stephen A.\ Butterfill}
\def \iemail{s.butterfill@warwick.ac.uk}
%\date{}

\input{$HOME/Documents/submissions/preamble_steve_paper3}
%\author{}
%\date{}



%\setromanfont[Mapping=tex-text]{Sabon LT Std} 

\begin{document}

\setlength\footnotesep{1em}

\bibliographystyle{$HOME/Documents/submissions/mynewapa} %apalike

\maketitle
%\tableofcontents
\title{}

\begin{abstract}
\noindent
***

\end{abstract}

\section{Shared Intention}
Why, if at all, is a notion of shared intention needed? 
This question is standardly answered by appeal to contrast cases \citep[compare][p.\ 150]{Bratman:2009lv}.
Thus \citet{gilbert_walking_1990} contrasts our intentionally walking together with two people who happen to be walking side by side. 
And \citet{Searle:1990em} contrasts park visitors who  simultaneously run to a central shelter in performing a dance with park visitors who likewise run to the central shelter but only because of an impending storm.
%And Bratman (\citeyear{Bratman:1992mi}, p.\ 333; \citeyear{Bratman:1993je}, p.\ 103--4) contrasts two people who cooperatively plan to go to New York  together with two people who each have an intention that they go to New York together by way of coercing the other into travelling with her.
These and other contrast cases invite the question, 
How do cases involving shared agency differ from cases involving parallel agency only? 

The first contrast case, Gilbert's, shows that the difference can’t be just  a matter of coordination because people who are merely happen to be walking side by side each other also need to coordinate their actions in order to avoid colliding.  
Note also that in both cases each individual's walking is intentional, so our intentionally walking together cannot be  only a matter of our each intentionally walking.
The second contrast case, Searle's, shows that the difference can’t just be that the resulting actions have a common effect because merely parallel actions can have common effects too.%
\footnote{
This use of contrast cases resembles \citet{Pears:1971fk}: he uses contrast cases to argue that whether something is an ordinary, individual action depends on its antecedents. 
} 
Perhaps, then, a notion of shared intention is needed to distinguish the two cases.  
Perhaps it is our acting on a shared intention that we walk together which distinguishes us from two strangers who happen to be walking side by side.%
\footnote{
Many philosophers agree that a notion of shared intention is useful for understanding acting together. 
Compare \citet[p.\ 5]{Gilbert:2006wr}: `I take a collective action to involve a collective intention.'  See also  
	\citet[p.\ 381]{Carpenter:2009wq}, 
	\citet[p.\ 369]{Call:2009fk}, 
	\citet{Kutz:2000si}, 
	\citet[p.\ 117]{rakoczy_pretend_2006} and 
	\citet{Tollefsen:2005vh}.
	}
	

But what could shared intention be?
In an influential series of papers,\footnote{ 
See \citet{Bratman:1992mi,Bratman:1993je,Bratman:1999fr,Bratman:2009lv}.
For influences beyond philosophy, see e.g.\ \citet{Tomasello:2005wx} and \citet{Knoblich:2008hy}. 
}
Bratman claims that the following are collectively sufficient\footnotemark \ conditions for you and I to have a shared intention that we J:
%
\footnotetext{
In \citet{Bratman:1992mi}, the following were offered as jointly sufficient \textit{and individually necessary} conditions; the retreat to sufficient conditions occurs in \citet[][pp.\ 143-4]{Bratman:1999fr} where he notes that `for all that I have said, shared intention might be multiply realizable.'
} 
%
\begin{quote}
\label{quote:bratman_account}
`1. (a) I intend that we J and (b) you intend that we J
 
`2. I intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb
 
`3. 1 and 2 are common knowledge between us' \citep[][p.\ View 4]{Bratman:1993je}
\end{quote}
%
In this paper we give a counterexample to Bratman's  claim that the above conditions, (1)--(3), are collectively sufficient conditions for shared intention. 
We shall also suggest a revision to avoid the counterexample.
%Apart from improving our understanding of what shared intention is, this will also bear on the kinds of planning that are involved in acting together.


Before going further we must distinguish two versions of the claim that (1)--(3) are collectively sufficient for shared intention.
The \emph{weak claim} is that there is some J such that these conditions are sufficient for you and I to intend that we J.
The \emph{strong claim} is that for any J, these conditions are sufficient for you and I to intend that we J.
One of Bratman's aims is to show that an account of shared intention can be \emph{conceptually conservative}:
he aims, that is, to show that it is possible to give an account of shared intention using concepts that `are available within the theory of individual planning agency' \citep[p.\ 163]{Bratman:2009lv}.  
Achieving this aim would require the strong claim, and it is to the strong claim that our counterexample is directed. 





How could we show that our meeting Bratman's conditions, (1)--(3), is not in fact sufficient for us to have a shared intention? 
We seek a case where the conditions are met although we lack a shared intention.
But how could we determine that we lack a shared intention?
As already mentioned, the notion of shared intention is supposed to 
	make it possible to 
	characterise systematically a difference between 
		cases involving shared agency (such as our walking together)
		and
		cases involving parallel agency only (such as two strangers who happen to be walking the same route side-by-side). 
Suppose, then, that we had a trio of cases, A, B and C, each involving two agents.
Suppose, further, that A and B involved parallel agency only, whereas C involved shared agency.
Then we could be sure that A and B do not involve shared intention.
Now suppose 
	that, for some relevant J, Bratman's conditions, (1)--(3) above, were met in cases B and C alike, 
	that in each case the structure of intention and knowledge played an appropriate role in guiding the agents' actions,
	and that in each case the agents thereby successfully J.
Then case B would be our counterexample:
Bratman's conditions are met but there is no shared intention.
This is how our counterexample will work.

Several preliminaries are necessary for the construction of our counterexample. 
These preliminaries might easily give the impression that our counterexample depends on an artificial settings. 
However, having introduced the primary counterexample in an artificial setting, we will  go on to show that counterexamples can also be constructed for mundane activities including walking together.


%%
%%two agents who intentionally J together (e.g.\ walk  together) and two agents who also  J together but do not do so intentionally (e.g.\ they happen to be walking side by side). 
%%
%This has two consequences.
%First, it should be impossible to construct a pair cases, A and B,
%meeting these criteria:
%	(i) in each case, two agents act intentionally;
%	(ii) in the first case, the two agents do not meet conditions (1)--(3) above;
%	(iii) in the second case, the agents do meet conditions (1)--(3) above;
%	(iv) the cases, A and B, are otherwise as similar as possible;
%	and 
%	(v) neither A nor B exemplifies the sort of shared agency gestured at by appeal to the contrast cases we started with.
%Second, it should also be impossible to construct a pair cases, B and C,
%meeting these criteria:
%	(i$^\prime$) in each case, two agents act intentionally;
%	(ii$^\prime$) in each case, the agents do meet conditions (1)--(3) above;
%	and 
%	(iii$^\prime$) C but not B exemplifies the sort of shared agency gestured at by appeal to the contrast cases we started with.
%
%
%
%
%Let's stipulate that two cases \emph{differ with respect to shared agency} when they differ in the way exemplified by the contrast between our intentionally walking together and our merely walking in parallel.
%Suppose, then, that we construct three cases, A, B and C.
%The three cases are as similar as possible, except that the above conditions are met in cases B and C but not in case A.
%Now if Bratman's conditions are sufficient for shared intention, A should differ from B with respect to shared agency, and B should not differ from C with respect to shared agency.
%But suppose that in fact A does not differ from B with respect to shared agency, whereas B does so differ from C.%
%\footnote{
%Strictly speaking only A and B are needed for the counterexample, of course.
%But including C makes it clearer that A and B do not differ with respect to shared agency.
%}
%Then we can conclude that Bratman's conditions are not sufficient for shared intention.
%This is how our counterexample will work.


\section{Neutral with Respect to Shared Intentionality}
Before we can introduce the counterexample 
we need to highlight a feature of Bratman's account 
	which we shall be exploiting.

Consider the contents of the intentions concerning our J-ing in the above clauses, (1)--(3). 
What sort of activity can you intend when you intend that we J?
We cannot restrict possible values of J to activities which involve shared agency.
In imposing any such restriction we would be assuming the very notion that an account of shared intention is supposed to illuminate. 
%
%As we have stressed, 
%	make it possible to 
%	characterise systematically a difference between 
%		cases involving shared agency (such as our walking together)
%		and
%		cases involving parallel agency only (such as two strangers who happen to be walking the same route side-by-side). 
%We must therefore avoid tacitly appealing to this distinction by  restricting possible values of J to those which involve shared agency. 
Rather the above  conditions, (1)--(3), must be sufficient for shared intention even for some values of J which are `neutral with respect to shared intentionality'.%
\footnote{
 \citet[p.\ 147]{Bratman:1999fr}.
 This refines Bratman's earlier view that some admissable values of J are cooperatively neutral 
 	where an  act-type is \emph{cooperatively neutral} just if `joint performance of an act of that type may be cooperative, but it need not be' \citep[p.\ 330]{Bratman:1992mi}. 
}

A consequence is that, in the right situations, one of us can rationally intend that we J, and can intend this unilaterally, that is without depending on anyone else intending that we J. 
Suppose you know that I am going to Chicago via a certain route at a particular time, 
and that I will do this regardless of what you do.
Suppose also that you can rationally intend that you go to Chicago in the same manner,
and that you know that if you act on this intention the upshot will be that we will go to Chicago together (although I may not know that we are going together---perhaps you will conceal your presence from me).
Then you can rationally intend that we go to Chicago together, 
	and you can intend this irrespective of whether I have any corresponding intention---providing, of course, that in so intending you are conceiving of our going to Chicago together in a way that is neutral with respect to shared intentionality.
What follows depends on the premise that this is indeed possible.%
\footnote{ 
\citet{Bratman:1999fr}  defends this claim at length. 
Note also that this claim must be true if Bratman's account of shared intention is to provide an informative and systematic distinction between the contrast cases mentioned at the start.
}


\section{Unshared Intentions}
As a further preliminary we need to introduce a definition.
Let us stipulate that we have an \emph{unshared intention} that we <J$_1$, J$_2$> where J$_1$$\neq$J$_2$ just if:
%
\begin{quote}
\label{df:unshared_intention}
1. (a) I intend that we J$_1$ and (b) you intend that we J$_2$
 
2. I intend that we J$_1$ in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J$_2$ in accordance with and because of la, lb, and meshing subplans of la and lb
 
3. 1 and 2 are common knowledge between us.
\end{quote}
In defining unshared intention we have used conditions exactly like Bratman's sufficient conditions for shared intention except that Bratman's conditions have J$_1$$=$J$_2$.
At this point it might be natural for readers to suppose that agents could not have unshared intentions,
or at least that they could not do so without irrationality.
In this section we describe a possible situation in which two agents  have an unshared intention without irrationality, deception or even ignorance.
This possible situation is not the promised counterexample, but it does form the basis for it.

Let us first introduce the activity we shall focus on.
Ayesha and Benji  are playing a simple video game which involves moving a cross around a two-dimensional space littered with barriers.
Ayesha can only accelerate the cross backwards or forwards,
while Benji can only accelerate it left or right. 
The cross moves around and interacts with the barriers in ways both players can predict.
The players are given tasks independently. 
These tasks always involve making the cross hit a target within two minutes of starting. 
A player succeeds when the cross hits her target, regardless of what happens to the cross afterwards.  
(It may go on to hit another target.)
In this case, 
	Ayesha's task is to make the cross hit the red square
	while
	Benji's task is make the cross hit the blue circle. 
In general it is possible that either or both will succeed, or that they will both fail.
Each movement carries a small cost to the player who moves, so that Ayesha and Benji each attempt to minimize how much he or she moves the cross consistently with completing his or her task.
At the outset, 
Ayesha and Benji are each neutral on whether the other succeeds or fails.
They are not opponents and do not seek to undermine each other's efforts, but each is entirely concerned  with his or her own task.
All of this is common knowledge for Ayesha and Benji.
They both know who has which task, what constraints they face and what their motives are.

Consider the possibility of one player intending, unilaterally, that the two players do something.
Suppose that one of the players---Ayesha, say---can knowledgeably predict that if she performs a certain sequence actions, <a$_1$, a$_2$, ...\ a$_n$>, then Benji will simultaneously perform certain other actions, <b$_1$, b$_2$, ...\ b$_n$>,
 and the upshot will be that the cross hits the red square.
Were this to happen, it would be true that Ayesha and Benji made the cross hit the red square.
Suppose, further, that Ayesha can intend to perform those actions <a$_1$, a$_2$, ...\ a$_n$>.
Then Ayesha can intend, unilaterally, that they, Ayesha and Benji, make the cross hit the red square.

Unshared intentions require a kind of symmetry.
Let us suppose that the above sequences of actions,
	Ayesha's <a$_1$, a$_2$, ...\ a$_n$>  and 
	Benji's <b$_1$, b$_2$, ...\ b$_n$>,
will also result in the cross hitting the blue circle. 
(Since the cross has momentum, we can suppose that it will hit both the red square and the blue circle at some time after these action sequences have been performed.)
Then by the reasoning just offered, Benji could intend that they, Ayesha and Benji make the cross hit the blue circle.
So Ayesha and Benji could meet the first condition, (1), for having an unshared intention.

What about the second condition, (2)?
Suppose that Ayesha knows two further things.
First, that Benji intends that they, Ayesha and Benji, make the cross hit the blue circle.
Second, that in acting on his intention Benji will perform actions <b$_1$, b$_2$, ...\ b$_n$>.
Then Ayesha can intend that they, Ayesha and Benji, make the cross hit the red square in accordance with and because of her intention that they make the cross hit the red square and in accordance with and because of Benji's intention that they make the cross hit the blue circle.

This is not quite enough to meet the second condition, (2), because there is also a requirement about meshing subplans. 
To make apply this requirement we need to generalise Bratman's definition of meshing:
\begin{quote}
`our individual subplans concerning our J-ing \emph{mesh} just in case there is some way we could J that would not violate either of our subplans but would, rather, involve the successful execution of those subplans' \citep[p.\ 106]{Bratman:1993je}.
\end{quote}
A natural generalisation is this:
\begin{quote}
our individual subplans concerning our <J$_1$, J$_2$>-ing \emph{mesh} just in case there is some way I could J$_1$ and you could J$_2$ that would not violate either of our subplans but would, rather, involve the successful execution of those subplans. 
\end{quote}
%
To illustrate, 
there would be a failure to mesh if, in intending that they make the cross hit the red square,  Ayesha's plans had included pushing Benji out of the way and seizing his controls. 
There would also be a failure of mesh if Ayesha were planning to trick Benji into a situation where we would be unable to perform the actions he had been planning.
But in the case we have been describing there are no such failures to mesh.
Each agent's subplans involve manipulating his or her own controls,
and the successes each seeks in doing this depends on the other successfully carrying out their subplans.
So Ayesha can rationally intend that they, Ayesha and Benji, make the cross hit the red square in accordance with and because of their intentions and meshing subplans of them.
And Benji likewise for making the cross hit the blue circle.

The only outstanding requirement for Ayesha and Benji to have an unshared intention is that their various intentions are common knowledge. 
Assuming common knowledge is possible where agents have a shared intention, it is likewise possible in this case of unshared intention.
So Ayesha and Benji can have an unshared intention that they <J$_1$, J$_2$> where J$_1$ is Ayesha and Benji's making the cross hit the red square and J$_2$ is their making the cross hit the blue circle.

So far we have shown that it is possible for two agents to have an unshared intention without irrationality, deception or ignorance.
Of course unshared intentions may be rare. 
But what matters for our counterexample is just that they are possible. 



%\section{Pure Unshared Intentions}
%We stipulate that an unshared intention is \emph{pure} if having that unshared intention does not involve having any shared intention. 
%It may be tempting to assume that all unshared intentions are pure. 
%This is not obvious, however, and may not even be true.
%Since Bratman provides only sufficient conditions for shared intention, his account doesn't tell us that an unshared intention is not a shared intention.
%For all Bratman says, the conditions defining unshared intention might also be collectively sufficient conditions for shared intention. 
%
%But doesn't shared agency require that there be a single activity, J-ing, about which each the agent involved has an intention? 
%This, too, is not obvious.  It might reasonably be denied by those who, like Bratman, reject the Simple View according to which when an individual intentionally J-s she has an intention concerning her J-ing \citep{Bratman:1984jr}.
%So the purity of an unshared intention cannot be assumed without argument. 
%This is why we do not rely on this assumption in what follows.
%
%%
%How can we show that Ayesha and Benji's unshared intention that they <J$_1$, J$_2$>  is pure?
%Perhaps we can appeal to intuition.
%Ayesha sees Benji's actions as constraints on her own, or else as opportunities.
%She exploits Benji's intentions for her own ends.
%Of course the situation is reciprocal: Benji exploits Ayesha in equal measure.
%Each allows himself or herself to be exploited by the other because being exploited enables exploiting,
%and this is the full extent of their cooperation. 
%Perhaps it is clear enough that 
%the sort of shared agency that an account of shared intention is supposed to capture must involve more than this sort of reciprocal exploitation, where each agent sees the other's actions only as constraints or opportunities.%
%\footnote{
%Note that we are not suggesting that reciprocal exploitation is incompatible with shared agency.  
%The claim under consideration here is rather that shared agency requires more than reciprocal exploitation.
%} 
%If so, we can already claim that Ayesha and Benji have a pure unshared intention.
%
%
%
%
%
%We started this paper by explaining the need for a notion of shared intention by appeal to contrast cases (following Bratman). 
%It is contrast cases that are supposed give us a pre-theoretical handle on shared intention.
%So to show that Ayesha and Benji's unshared intention is pure, we need to construct a contrasting case.
%
%***Lily and Isabel, 
%	like many people, 
% 	sometimes attribute intentions and other states to imaginary agents, and perform actions on their behalf. 
%In some cases this enables them to further their own real-world objectives (as in `Teddy wants to go to the park now').	
%Some of these imaginary agents are toys. 
%But some are imaginary aggregate agents. 
%Sometimes, that is, Lily and Isabel imagine that there is an agent distinct from either of them and such that all its parts are parts of them.
%They attribute intentions and other states to this agent, and act on its behalf.
%
%
%


%
%An account of shared intention should enable us to  distinguish systematically between two types of case in which we J together, one where we intentionally J together and the other where  our J-ing together involves  merely parallel actions.
%As noted, Gilbert's example of walking together shows that merely parallel actions can be mutually responsive, as when two strangers walking side by side avoid collision thanks to meshing subplans.



\section{The Counterexample}
%*Need a beefier activity, something more than cooperatively neutral.

In the situation just described,
Ayesha and Benji are playing a game and have different tasks.
Thanks to special features of the game environment, they both succeed by acting on an unshared intention.
Now compare two further players, Yasmin and Zak, who are playing the same game. 
Their situations, knowledge states, intentions and actions are as similar as possible to Ayesha's and Benji's except for one detail.
Just by chance they have been assigned identical tasks: Yasmin's task is to make the cross hit the red square and Zak's task is the same.
So where Ayesha and Benji have an unshared intention that they <J$_1$, J$_2$>,
Yasmin and Zak meet Bratman's conditions (1)--(3) for having a shared intention that they J$_1$.  
But Yasmin, in planning and acting, does not rely on the coincidence of their intentions; and nor does Zak.
(Yasmin relies on the fact Zak intends that they J$_1$, of course; but she does not rely on the fact that Zak intends what she intends.)
Furthermore, due to an artefact of the way the game is structured,
the unshared intention and the Bratman intention (as we might label the structure of intention and knowledge while leaving open whether it constitutes a shared intention), result in the two pairs performing same actions in the same way.
That is, Yasmin reasons about Zak much as Ayesha reasons about Benji and Yasmin and does what Ayesha does, and likewise for Zak and Benji. 


We claim that Yasmin and Zak have a shared intention that they J$_1$
only if 
Ayesha and Benji have a shared intention.%
\footnote{
Strictly speaking, 
	what matters for our argument is whether or not Ayesha and Benji have a shared intention \emph{in virtue of having the unshared intention that they <J$_1$, J$_2$>}.
	This is because, strictly speaking, we need to show, not that Yasmin and Zak lack any shared intention whatsoever, but only that they lack a shared intention that they J$_1$ in virtue of meeting Bratman's conditions. 
	For ease of exposition this is not made explicit in the main text.
} 
This claim follows from the similarities of the two cases.
The only difference is that Yasmin and Zak happen to be assigned the same task, whereas Ayesha and Benji are not.
And neither Yasmin nor Zak makes use of the fact that they have the same task. 
(This is not due to ignorance: it's just how they choose to approach their tasks.)
So if we consider on how  
		the case of Yasmin and Zak
	differs from
		 that of Ayesha and Benji,
we can see that these differences do not plausibly amount to a difference with respect to shared agency.
Shared intention cannot feature in one case but not the other.

To show that the case of Yasmin and Zak do not have a shared intention it remains only for us to show that Ayesha and Benji do not have one. 
Here we must be careful.
First note that,
	 since Bratman provides only sufficient conditions for shared intention, 
	 his account doesn't tell us that an unshared intention is not a shared intention.
For all Bratman says, the conditions defining unshared intention might  be sufficient for shared intention. 

But doesn't shared intention require at least this much,
that there be a single activity  about which each the agent involved has an intention? 
This might reasonably be doubted by those who, like Bratman, reject the Simple View according to which when an individual intentionally F-s she has an intention concerning her F-ing \citep{Bratman:1984jr}.
So 
	we shall not infer  that Ayesha and Zak lack a shared intention
	just because (by construction) there is no F such that Ayesha and Zak's each intend that they, Ayesha and Zak, F.

Can we then appeal directly to intuition to show that Ayesha and Zak lack a shared intention?
Ayesha sees Benji's actions as constraints on her own, or else as opportunities.
She exploits Benji's intentions for her own ends.
Of course the situation is reciprocal: Benji exploits Ayesha in equal measure.
Each allows himself or herself to be exploited by the other because being exploited enables exploiting,
and this is the full extent of their cooperation. 
We don't suppose that reciprocal exploitation is incompatible with shared intention. 
But Ayesha and Zak's interaction consists entirely in this sort reciprocal exploitation, where each agent sees the other's actions only as constraints or opportunities.
Perhaps it is clear enough that 
the sort of shared agency that an account of shared intention is supposed to capture must involve more than this. 
If so, we can already claim that Ayesha and Benji are not acting on a shared intention.
But philosophers' intuitions about shared agency may not be entirely, so it would be better if we could avoid such a blunt appeal to intuition.

How else could we support the claim that Yasmin and Zak lack a shared intention? 
As mentioned at the start, 
the contrast cases are often used to anchor intuitions in theorising about shared agency.
In the next section we shall further support our claim by contrasting Yasmin and Zak's case with a further case, one which is as similar as possible and which does seem to involve shared agency.
The fact that Yasmin and Zak's case contrasts with this new case will support the claim that Yasmin and Zak lack shared intention.
This is the aim of the following section.
A first motive for introducing this new contrast case is that it provides an indication concerning what is missing from Bratman's account, and so may help us to understand why his conditions are not sufficient for shared intention.



\section{Agent-neutral Plans}

Above we introduced what will turn out to be a counterexample to Bratman's account, the case of Yasmin and Zak. 
But we have yet to show that this case really is a counterexample. 
For all we have said so far, a proponent of Bratman's view might insist that Yasmin and Zak do have a shared intention.
To show that they do not,
we shall contrast Yasmin and Zak's case with a third case that is as similar as possible but does involve shared intention. 

%---the following is a mistake (because Yasmin and Zak do take each other's task into account.
%It is unlikely that humans would spontaneously behave as Yasmin and Zak do.%
%\footnote{
%This is nicely illustrated in a series of experiments by Sebanz and colleagues in which subject spontaneously take another's plans into account in planning their own actions, even where doing so is manifestly not required and can impair their performance  \citep{Sebanz:2003kf,Sebanz:2005fk,tsai:2011_groop_effect}.
%}

First we need to introduce the notion of an agent-neutral plan.
We stipulate that a planning process, or a plan, is \emph{agent-neutral} just if it does not involve identifying any particular agents.  
This sort of planning is quite common.
For example, some housemates who have decided to take on an allotment to grow vegetables might sit down together to plan what needs doing without yet assigning roles to particular individuals. 
In so planning, each housemate is thinking about what is to be done and not what she herself will do.  
(This is an idealisation, of course.)
At some point the planning is done  and they divide up the roles.
Of course they may not find a way of dividing up roles that everyone is prepared to go along with---individuals' preferences, abilities and intentions may block the plan's adoption.
But suppose the housemates do divide up the roles in a way that is acceptable to everyone, 
	and that each implements her part in the plan.
Then each conceives of her own and the others' actions as part of single plan directed to achieving a single outcome.


In the above example of agent-neutral planning,
	the housemates plan together and agree on a common plan.
	Planning together is plausibly an activity which involves shared agency.
	Note, however, that an individual can construct an agent-neutral plan by herself, even if its eventual execution will involve others.
	In fact, two or more individuals who are assigned a task might each individually engage in agent-neutral planning in parallel.
	The task demands and their planning strategies may conspire to ensure that they each come up with the same agent-neutral plan, or at least that their plans are compatible.%
\footnote{
Suppose that, for some outcome, two or more agents each have a plan for the realisation of that outcome (these plans are possibly but not necessarily agent-neutral).
Then their plans are \emph{compatible} just if 
	there is a way of assigning agents to any unspecified roles in each plan such that  each agent could rationally act on the intention that they realise the outcome by 	performing the role she is assigned in her own plan even though all facts about which agents are performing which roles in which plans were common knowledge to the agents.
To illustrate, suppose that our task is to press a button simultaneously. 
If your plan specifies that the agents start to move in exactly 60 seconds and press the button 5 seconds later whereas my plan specifies only that there is a leader and the agents are to press the button 5 seconds after the  leader starts moving, then our plans are compatible.
}
%
Similarly, even if the agents continue to operate individually and without communicating or otherwise intentionally sharing information, the task demands and manifest properties of the agents, such as their distribution in space, may ensure that, having arrived at their plans, each agent assigns the same roles to the same individuals (or at least that each agent assigns roles in such a way that the resulting agent-specifying plans are compatible).
%
Finally, each agent may know enough about herself and the others to be able to determine, without communicating, whether the plan and role assignments will be acceptable to everyone. 
	And all of this---that they engage in parallel agent-neutral planning resulting in compatible plans and role assignments, which are acceptable to all---may be common knowledge to the agents.
	So it is possible, in principle at least, 
	that several agents might each individually engage in agent-neutral planning and rationally perform their part in the resulting plan, knowing that the others will do likewise.
	Parallel agent-neutral planning can rationally result in coordinated action without requiring shared agency.

Now that we have some background on agent-neutral plans,
let us introduce a third and final case.
This case needs to be as similar as possible to that of Yasmin and Zak's while involving shared agency.

Lily and Isabel start in the same situation as Yasmin and Zak. 
Each is tasked with making the cross hit the red square (J$_1$). 
Once again each cares only about her own success at the outset. 
%---following version is for shortening, where the above background is skipped
%Lily takes the view that the best way for her to succeed is to engage in a kind of agent-neutral planning.
%A planning process is \emph{agent-neutral} when it does not involve identifying any particular agents.
%This sort of planning is quite common; for example, some housemates, having decided to go camping, might sit down to plan what needs doing without yet assigning roles to particular individuals.
%Similarly, Lily, knowing that she and Isabel have the same task,
%plans how two agents in their situation could J$_1$.  
Lily, knowing that she and Isabel have the same task, takes the view that the best way for her to succeed is plan how two agents in their situation could J$_1$.  
So Lily ends up with an agent-neutral plan that specifies both her own actions and Isabel's actions without identifying her own actions as such (and without identifying Isabel's actions as Isabel's either).
Having done this, Lily then assigns one role in the plan to herself and the to Isabel.
At this point Lily considers whether she would be prepared to go along with the plan given her intentions, preferences and values, and she also considers whether Isabel would be prepared to go along with it too. 
In this case it happens that both would be prepared to go along with the plan.
Lily then knowledgeably predicts that Isabel, who has similar planning abilities and has been approaching their task in a similar way, will have made a compatible plan.
Lily therefore attempts to carry out her part in the plan she made; and Isabel does likewise.

In short, then, Lily and Isabel are like Yasmin and Zak in nearly every respect. 
Each pair has a Bratman intention that they J$_1$, each pair acts on this intention and each pair ends up performing the same sequence of actions.
The difference is just that Yasmin and Zak make no use of the fact that they are performing the same task.
	This fact does not show up in their planning. 
	Rather, each plans her own actions only and treats the other's actions as constraints to work around or opportunities to exploit.
By contrast, Lily and Isabel embrace the fact that they are performing the same task. 
They may not like having to act together; in fact each may far prefer to act alone were that possible.
And, like Yasmin and Zak, they are unconcerned with each other's success except insofar as their own success depends on it.
But Lily and Isabel nevertheless make use of the fact that they have to perform a single task together by each constructing a single plan covering both of their actions and then carrying out their parts in these plans.
So why does Lily and Isabel's case, but not Yasmin and Zak's case, plausibly involve shared agency?
The reason is  this:
	that at some stage of the planning which rationally guides and coordinates their actions,
	Lily and Isabel each conceive of their actions as part of a single plan directed to achieving a single outcome.


Given that Yasmin and Zak's case contrasts with Lily and Isabel's,
we conclude that Yasmin and Zak do not have a shared intention.
As explained above,
 Yasmin and Zak do meet Bratman's conditions for shared intention, and do act appropriately on the corresponding intentions and knowledge. 
 So Yasmin and Zak's case  is a counterexample to sufficiency of Bratman's conditions for shared intention.


Note that in describing the case of Lily and Isabel we have not introduced contralateral commitments or other elements foreign to Bratman's account of shared agency. 
Of course, we have not shown that these are not necessary for shared agency; 
	but if we knew that they were necessary we would not need a counterexample. 
Our counterexample is useful because it draws on the same planning resources Bratman's account draws on, with just one addition.
The addition is the idea that agents can conceive, or fail to conceive, of their actions as part of a single plan. 

Given that Yasmin and Zak's case is a counterexample,
there are also many further counterexamples involving mundane activities and less elaborate props.
Here, for instance, is how Yasmin and Zak walk together.
They are firmly tied at the ankle, and neither is strong enough to move without the other.
Yasmin needs to get to the corner, and so does Zak.
Further, they have a Bratman intention that they walk to the corner, and they act on this intention in walking to the corner.
Again, this is not a case of shared agency.
The problem is not that Yasmin and Zak are tied together against their will; 
after all, many cases of shared agency are involuntary in this sense.
The problem is rather that each conceives of the others' intentions and actions only as constraints and opportunities.
They fail to make any use of the fact that they intend the same outcome,
 and so they fail see their actions as part of a single plan.

Yasmin and Zak's attitude may be unnatural.
Perhaps humans in their situation would not normally ignore the fact that they have the same task in the way that these agents do.
This may be why Bratman's conditions appear, misleadingly, to be sufficient for shared intention.


\section{Revising Bratman's Account}
How could Bratman's account be revised to avoid the counterexample?
Perhaps we can simply add to Bratman's conditions.
Consider the view that these are sufficient condition for us to have a shared intention that we J:
%
\begin{enumerate}
\label{revised_account}
\item (a) I intend that we J and (b) you intend that we J.
 \item I intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb; you intend that we J in accordance with and because of la, lb, and meshing subplans of la and lb.
\item At some point in my planning, I produce a plan for all of our actions; and you do likewise.
\item Our plans are compatible and we (partially) implement these plans.
\item All of this, 1--4, is common knowledge between us.
\end{enumerate}
%
This view avoids the counterexamples we constructed,
but at the cost of a loss of generality.
For these conditions are only met once agents have not only done some planning but also started to act.
While the aim is not to provide necessary conditions for shared intention, we should aim for enough generality to illuminate some central cases.
So we shall not consider this view further.

For a different approach, consider activities which meet these conditions concerning some outcome G:
\begin{enumerate}
\item Each agent has a plan for how they, the agents, can G.  (This can be an agent-neutral plan plus an assignment of roles.)
\item Each agent's plan is acceptable to every agent (it does not conflict with any individual's preferences, abilities or intentions).
\item The agents' plans are compatible.
\item Each agent performs the roles assigned to her in her own plan, and the agents thereby G. 
\end{enumerate}



%
%*** each thinks what \emph{we} should do.  The fact that they have the same task shows up in their planning.
%
%
%***Simple point: the fact that we have the same task should play a role in our planning.  It is something that we should embrace, and perhaps even want.  (Be careful: two people might intentionally act together although each would prefer to act alone, and would seize the opportunity to do so.)  Cordula's idea (modified) was that we aim for shared success (it's not a question of whether I care about \\emph{your} success, but I must care about \emph{our} success).


\section{Conclusion}
Bratman's conditions are not sufficient for shared intention because they leave open the possibility that  when two or more agents meet his conditions, each fails to rely on the fact that there is a single outcome to which all of their actions are directed and so conceives of the others' actions merely as constraints and opportunities.
But Bratman's conditions appear to be sufficient for shared intention because it is natural that when two or more agents meet his conditions, they would rely on the fact that their is a single outcome to which all of their actions are directed and therefore be disposed to conceive of all their actions as parts of a single plan. 
The case of Yasmin and Zak shows that although this may be natural, it not necessary.



\section{***OLD}
Assuming for a moment that this is right,
what might explain the difference between Yasmin and Zak on the one hand and cases like Gilbert's two people intentionally walking together on the other hand?
To answer this question in the spirit of Bratman's planning-theoretic approach, we need to step back and think about planning generally.


Imagine observing Ayesha and Benji as they play the game. 
Suppose you decide to make a plan of action for one or both of them.
In doing this you could either adopt one player's perspective or you could adopt a fictional shared perspective.
Adopting one player's perspective, Ayesha's say, 
	you think about what she might do to achieve her task,
	predict how this will influence Benji's actions,
	and then feed  these predictions back into the question of what Ayesha might do.
From this perspective, Benji's actions are constraints to work around and opportunities to exploit.
Alternatively you might adopt a fictional shared perspective, 
	 mentally replacing Ayesha and Benji with a single, imaginary aggregate agent who has to perform both tasks alone.
	 In this case, you think about what combination of actions would best achieve both tasks and then, right at the end, you assign these actions to Ayesha and Benji.
From this perspective, it is as if there is a single mind behind the plan.

Note that the number of agents involved does not dictate which perspective we can take.
Just as we could take a fictional shared perspective where two or more agents are involved, 
so also we could take a fictional split perspective on the actions of  a single agent, and even of ourselves.
The newly elected  politician with a young family and working partner might see herself almost as if she were two people engaged in different projects while forced to shared a single body. 

Shared perspective of a fictional aggregate agent.
Can be a matter of negotiation what the fictional aggregate agent's values, preferences and intentions are.

Perhaps the reason why Yasmin and Zak fail to have a shared intention (if they do---this has not yet been argued) is that each plans entirely from her own perspective. 
Maybe shared intention sometimes requires that agents be disposed to plan at least some of their actions from a shared perspective.

Now consider a third (and final) pair of players, Lily and Isabel.
Lily and Isabel each take the appropriate shared perspective, as if imagining a fictional aggregate agent who had been assigned their two tasks, 
and they plan accordingly.
And this is common knowledge.
What matters is not what they do, however.
It is that the J which they intend is characterised in terms of planning from a shared perspective.
What they intend when they intend that they J does not leave it open whether they will adopt a shared perspective in planning their J-ing.
J is neutral with respect to shared intention but it is not neutral with respect to the requirement that each agent individually adopt a shared perspective when planning how to J.
***HERE



Imagine two tasks need to be completed; perhaps you need to buy some shoes and get a haircut. 
Which actions you perform in attempting the first task constrains what actions you can perform in attempting the second task, and conversely.
For instance, if you shop for the shoes in one store, you won't have enough money left for the best hairdressers.
There are two ways to approach planning these tasks.
One approach involves treating the two tasks as separate.
Perhaps you first think of a plan that is optimal for buying shoes.
Then, taking this plan as a constraint, you attempt to plan the haircut.
But as it happens you discover that the options for the haircut are unsatisfactory.
So you revise the shoe-buying plan and try again to plan the haircut.
A different approach to planning would be to treat the two tasks as part of a larger, single task.
Perhaps you identify several possible combinations of shoe-buying and haircut-getting actions.
You then ask yourself which combination is best overall and then perform the component actions.




Take a situation like the game Ayesha and Benji are playing but now imagine that you are taking over from both players and intend to perform both tasks. 
One way to plan would involve thinking of two separate tasks.
First you think about the best way to make the cross hit the red square.



%
%The difference separating Ayesha and Benji, on the one hand, from Yasmin and Zak on the other 
%	is unlike the difference separating two strangers who merely happen to be walking side by side from two people intentionally walking together.
%So if Ayesha and Benji's do not intentionally act together, then nor do Yasmin and Zak.


 

***Can get a parallel to the special case discussed here by considering walkers whose legs have been tied together in such a way that they are forced to accommodate each other if they are to get around.  Such people may walk together and meet the Bratman conditions without actually 




We shall argue that although Yasmin and Zak meet Bratman's conditions (1)--(3) for having a shared intention that they J$_1$, they do not have any such shared intention.
This, then, is our counterexample.
Of course it is probably not yet obvious that Yasmin and Zak do not have a shared intention.
To see that they do not we need to introduce one more case.

We started this paper by explaining the need for a notion of shared intention by appeal to contrast cases. 
An account of shared intention should enable us to  distinguish systematically between two types of case in which we J together, one where we intentionally J together and the other where  our J-ing together involves  merely parallel actions.
As noted, Gilbert's example of walking together shows that merely parallel actions can be mutually responsive, as when two strangers walking side by side avoid collision thanks to meshing subplans.



*Yasmin and Zak: it's like we've tied the Gilbert's walking strangers' ankles together, so that they can only succeed together.





***


Our argument is:
\begin{enumerate}[label=\roman*]
\item Yasmin and Zak have a shared intention that they J$_1$ only if Ayesha and Benji have a shared intention in virtue of having the 
 unshared intention that they <J$_1$, J$_2$>.
\item Ayesha and Benji's unshared intention that they <J$_1$, J$_2$> is a not shared intention.
\end{enumerate}
%
Therefore:
%
\begin{enumerate}[resume,label=\roman*]
\item Yasmin and Zak do not have a shared intention that they J$_1$.
\end{enumerate}
%
In the rest of this section we defend the two premises, starting with the second, (ii).

Could Ayesha and Benji's unshared intention that they <J$_1$, J$_2$> be a  shared intention?
This question might initially appear frivolous but it is not straightforward to answer.
Since Bratman provides only sufficient conditions for shared intention, his account doesn't tell us that the unshared intention is not a shared intention.
And while we might be tempted to assume that for two agents to intentionally J together they must at least each have intentions concerning J, 
this assumption might reasonably be rejected by those who, like Bratman, reject the Simple View according to which when an individual intentionally J-s she has an intention concerning her J-ing \citep{Bratman:1984jr}.

In defence of the second premise, (ii), consider that shared intentions are suppose to play a certain functional role.
Among other things, they are supposed to coordinate 







\section{Prior counterexamples}
***Must mention that Tollefsen and Gold \& Sugden counterexamples fail.


\section{The Counterexample}
Suppose that Aravinda runs the trains and Gerhard  the busses.
Let us stipulate that the extent to which the two services, train and bus, are coordinated is to be measured by the total time passengers spend waiting between a bus and a train.
So changes to the timetables would result in better coordination just if the changes would total waiting time.
To illustrate, improving coordination might involve having trains arrive at a station shortly before, rather than shortly after, busses depart from there.
Now Aravinda and Gerhard each intend that they, Aravinda and Gerhard, coordinate the trains with the busses to the greatest extent possible given other constraints; and they intend to do this by way of these intentions and meshing subplans of them, and this is common knowledge between them. 
Aravinda and Gerhard thus meet Bratman's sufficient conditions for them to have a shared intention.
Acting on this intention, 
every January Aravinda updates the train timetables and sends Gerhard the changes.
Likewise, Gerhard  updates the bus timetables every June and sends Aravinda the changes.
In this way  each is responsive to the other's intentions and subplans of these. 
Indeed, each may even try to predict changes in the other's subplans and modify their own accordingly.  
But from each individual's point of view, the other's plans are  merely a constraint.

***

So whether Gerhard succeeds relative to his intention depends on both his own and Aravinda's plans; and likewise for Aravinda.
%This is why each intends that they optimally coordinate the services in accordance with meshing subplans.
This is why each is responsive to the other's plans. 
Indeed, each may even try to predict changes in the other's plans and modify their own accordingly.  
But from each individual's point of view, the other's plans are  merely a constraint.
This approach to planning suffers from two defects.
First, it is unlikely to be optimal in the sense of resulting, eventually, in a combination of plans such that no other combination of plans would have been better for at least one service and no worse for either service.
Second, it is unlikely to be efficient in the sense of allowing Gerhard and Aravinda to arrive at an optimal combination of plans for the two services, trains and buses, with the fewest iterations.
How could they do better?
One possibility may be to have a single plan covering busses and trains---perhaps, for example, Aravinda could buy Gerhard's franchise. 
%But suppose that this is not possible, and that there are limits on how much information Aravinda and Gerhard can share. 
Now there is a single goal to which Aravinda and Gerhard's activities are both directed.
But suppose that this is not possible, and that they are unable to integrate their planning more tightly---practical constraints or regulation prevents them from opening up their planning to each other.
Can they still do better than treat each other's plans as a constraint?
Possibly.
Each plans the whole bus--train operation. 

New contrast case: Aravinda and Gerhard planning the whole thing vs. merely responding to each other's plans.
Meet Bratman's sufficient conditions for shared intention in both cases.
(Specifically, in both cases they act on a shared intention that they coordinate the trains with the busses to the greatest extent possible given other constraints.)
But intuitively is a case of coordinating the trains with the busses together whereas the other case involves Aravinda and Gerhard merely doing this in parallel.
So shared intention is insufficient to fully explain the contrast cases.

I'm also inclined to think it is not necessary for solving the contrast cases (distributive goals ...\ collective goals).
The contrast cases do not provide a firm anchor for theorising about shared intention.



\bibliography{$HOME/endnote/phd_biblio}



\end{document}